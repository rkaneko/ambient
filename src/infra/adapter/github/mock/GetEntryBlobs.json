{
  "data": {
    "repository": {
      "object": {
        "tree": {
          "entries": [
            {
              "type": "blob",
              "name": "README.md",
              "object": {}
            },
            {
              "type": "tree",
              "name": "contents",
              "object": {
                "entries": [
                  {
                    "type": "tree",
                    "name": "2016",
                    "object": {
                      "entries": [
                        {
                          "type": "tree",
                          "name": "12",
                          "object": {
                            "entries": [
                              {
                                "type": "tree",
                                "name": "17",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "notes_ui-router-for-react.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/d4e7224b91a7f168f63d9d35eb3fcc38569cc221",
                                        "text": "UI-Router for React\n===\n\n# New concepts\n\nいくつかのコンセプトとそれを実現するためのUI-Routerの機能を見ていく.\n\n## Resolve data\n\nユーザがSPAにおけるstateを行ったり来たりするときに,アプリケーションはREST endpointのようなサーバAPIを使ってdataを取得する必要がある場合がある.\n\n`resolve`プロパティを定義することで、stateは必要なdataを特定できる.ユーザが`resolve`プロパティを持つstateをactivateすると,UI-Routerはstateをactivateする前にデータをfetchする.fetchされたdataは`props`経由でcomponentに渡される.\n\n`resolve`\n\n```js\nresolve: [\n  {\n    token: 'people',\n    resolveFn: () => PeopleService.getAllPeople()\n  }\n]\n```\n\n- `resolve`は`array.<object>`.\n- arrayのElementにfetchされるべきdataをobjectで定義する.\n  - `token`プロパティはロードされたdataのためのDI token.\n  - `resolveFn`プロパティはdata取得のためのpromiseを返す関数.返されたpromiseから取得されたdataはDI tokenに代入される.\n  - `deps`プロパティには`resolveFn`の依存関係を定義する.`resolveFn`の関数の引数としてDI tokenを指定する.`array.<string>`で指定する.\n\nDI tokenについて\n\nソースを軽く読んでみた感じだと、`UIView`の子componentに[Reactのcontext](https://facebook.github.io/react/docs/context.html)の仕組みを使ってpropsに渡していっているみたい？詳しくはReact力が足りなくてわからなかった。\n\nhttps://github.com/ui-router/react/blob/9745555cff70dc1cb7fa510ada1e2644a21bfbd8/src/components/UIView.tsx\n\n`resolve`の`token`に`'people'`を指定し、`resolveFn`によって取得されたdataは各componentの`props`に渡される。\n\n```js\nPeople.propTypes = {\n  resolves: PropTypes.shape({\n    people: PropTypes.arrayOf(PropTypes.object)\n  })\n}\n```\n\n## State Parameters\n\nRESTのendpointがリソースに対する識別子などを持つ場合に,この情報を元にdataをfetchしたい場合がある。\n\n`/people/:personId` のようなURLで`personId`がpeopleの識別子を表す場合に,`personId`を使ってデータを取得したいようなケース.\n\npersonのstateは次のようになる.\n\n```js\nconst person = {\n  name: 'person',\n  url: '/people/:personId',\n  component: Person,\n  resolve: [{\n    token: 'person',\n    deps: ['$transition$'],\n    resolveFn: (trans) => PeopleService.getPerson(trans.params().personId)\n  }]\n}\n```\n\n`deps`プロパティに`'$transition$'`を指定することで,`resolveFn`に`Transition` objectがinjectされる.Transition objectを経由してURL resourceにアクセスすることができる.\n`Transition` objectは現在のhistoryの状態遷移についての情報を保持する特別なinjectableなobject.\n\nrefs: https://developer.mozilla.org/en-US/docs/Web/API/History_API\n\n## Linking with params\n\nUI-router for reactで管理するhistoryのstateに対して,ｈｔｍｌの`<a>`のようなtagではstatの変化eventを発火させることができない.そのために`<UISerf>`といったReact componentが提供されている.これを利用してrouterにstate変化event(ここでは,person detailページに遷移するというevent)を発火させる.\n\n```js\nconst ToPerson = (props) => {\n  return (\n    <UISerf to=\"person\" params={{ personId: props.person.id }}>\n      <a>{props.person.name}</a>\n    </UISerf>\n  );\n};\n```\n\n> あるstateから別のstateにスウィッチすることをTransitionと呼ぶ.\n\n## Nested States\n\nUI-Router statesは単一のroot stateから始まるTreeを形成する.root stateは暗黙的で名前を持たない.top-levelのアプリケーションstateは暗黙的なroot stateの子供となる.\n\n例えばpeople stateの子であるperson stateを次のように書き換えられる.\n\n```diff\nconst person = {\n-  name: 'person',\n-  url: '/people/:personId',\n+  name: 'people.person',\n+  url: '/:personId',\n  component: Person,\n  reolve: [{\n    token: 'person',\n-    deps: ['$transition$'],\n+    deps: ['$transition$', 'people']\n-    resolveFn: (trans) => PeopleService.getPerson(trans.params().personId)\n+    resolveFn: (trans, people) => people.find(person => person.id === trans.params().personId)\n  }]\n}\n```\n\n### State name\n\n`someStateName.anotherOneName`は`someStateName`の名前を持つstateと`anotherOneName`を名前に持つstateに親子関係を形成する.\n\n### URL\n\n親子関係を作ると,URLも親のstate urlからの相対urlで記述できる.\n\n### Resolve\n\n前提として,`person` stateの前には`people` stateにおいてserver APIから`peopel`を取得してから,その後で`person` stateに遷移する.routerは`person` stateをアクティベイトする前に`person` resolveを呼び出す.しかし,ここでの操作では少し異なっていて,`people` stateから取得していたデータを`person`のresolveにinjectしてからそのデータを利用する.サーバにfetchはしない.親のresolveによりpeopleリストが既にロード済みなため,追加のfetchが必要ない.\n\n> resolve関数では,親stateや同一stateのresolveの結果をinjectすることもある.\n\n### View\n\n\n"
                                      }
                                    }
                                  ]
                                }
                              },
                              {
                                "type": "tree",
                                "name": "25",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "learn_rxjs.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/650045888975eb2718003cbcaed1d9a9ec00373e",
                                        "text": "Learn RxJS\n===\n\n### Preconditions\n\n- RxJS ver5.x\n\n### materials\n\n- http://reactivex.io/rxjs/manual/overview.html\n\n---\n\n# Introduction\n\nRxJSはobservable sequencesを用いて非同期イベント駆動プログラミングを実現するためのライブラリ.\n以下を提供する.\n\n- Observable typeとそれに伴うObserver, Schedulers, Subjects type.\n- `map`, `filter`, `reduce`などの非同期イベントをコレクションのように操作するためのoperators.\n\nイベントシーケンスの理想的な管理を目指す.\n\nessential concept\n\n- `Observable`: まだ処理されていない値やイベントの呼び出し可能なコレクションの概念を表す.\n- `Observer`: `Observable`から受け取った値のどう処理するかの責務を担うコールバックのコレクション.\n- `Subscription`: `Observable`の実行を表す.キャンセル処理も行える.\n- `Subject`: EventEmitterと等価.値を複数のObserverにマルチキャストする唯一の手段.\n- `Schedulers`: 並列処理を管理するdispatcherの集合.`setTimeout`や`requestAnimationFrame`等が起こった時の計算処理をコーディネートすることができる.\n\n\n# Observable\n\n## Pull versus Push\n\nPullとPushはdata Producerがdata Consumerとどのようにコミュニケーションできるかということに関して異なるプロトコルである.\n\nPull型のシステムでは,ConsumerはいつProducerからdataを受け取るかを決めている.Producer自身はいつConsumerにdataが届くについては関心がない.\n\nすべてのJavaScript functionはPull system.functionはdata Producerである.\n\nES2015で提供されるgenerator functionとiteratorは別のタイプのPull systemと言える.`iterator.next()`はConsumerで,`iterator`(Producer)から複数の値をpullする.\n\n| |Producer|Consumer|\n|:---:|:---:|:---:|\n|Pull|Passive|Active|\n|Push|Active|Passive|\n\nPush型のシステムでは,ProviderがいつdataをConsumerに送信するかを決める.Consumerはいつdataが届くについては関心がない.\n\n今日のJavaScriptではPromiseがPush型システムの代表的な型と言える.Promiseは解決された値を登録されているコールバック関数に送信する.\n\nRxJSはJavaScriptに新たなPush型システムとしてObservableを導入する.*Observable\nは複数の値のProducerでそれらの値をConsumer(Observer)にpushする.*\n\n- `function`は呼び出し時に一つの値を同期的に返す遅延評価演算.\n- `generator`は呼び出し時にzeroから無限個の値を返す遅延評価演算.\n- `Promise`はいつか一つの値を返すか失敗するかの演算.\n- `Observable`は呼び出し時からzeroから無限個の値を同期的もしくは非同期的に返すことができる演算.\n\n## Observables as generalizations of functions\n\n一般的な主張とは反対に,ObservablesはEventEmitterでも複数の値のためのPromiseでもない.\nRxJS Subjectsを用いてEventEmitterのように振る舞うときもあれば,EventEmitterのように振る舞わないときもある.\n\nsee also observable-as-a-generator.js\n\nObservableは処理と実行を分けている.EventEmitterはSubscriberの存在有無に関わらず実行と処理内の副作用を共有してしまう.この点でObservableはEventEmitterとは異なる.\n\nObservableは同期的に一つの値を送信できるという点ではfunctionと同じことができる。しかしながら,functionにはできないことができる.\n\nsee also\n\n- observable-can-deliver-multi-values.js\n- observable-can-deliver-multi-values-async.js\n\n### Conclusion\n\n- `func.call()` は同期的にひとつの値を与える.\n- `observable.subscribe()` はいくつかの値を同期的にも非同期的にも与える.\n\n---\n\n## Anatomy of an Observable\n\n`Observable`は\n\n- `Rx.Observable.create`やcreating operatorによって生成される.\n- Observerによってsubscribeされ,ObservableからObserverにはObserverの`next`,`error`,`complete`メソッドを介して通知が伝達される.\n- Observableの実行はdispose(廃棄)される.\n\n### Creating Observables\n\n`Rx.Observable.create`は`Observable`のconstructorのalias.引数に`subscribe` functionを取る.\n\n```js\nconst observable = Rx.Observable.create(function subscribe(observer) {\n  const id = setInterval(() => {\n    observer.next('hi');\n  }, 1000);\n});\n```\n\n`create`によって`Observable`を生成できるが,通常は`from`や`interval`のようなcreation operatorを使うことが多い.\n\n### Subscribing to Observables\n\n生成されたObservableインスタンスは以下のようにして`subscribe`することができる.\n\n```js\nobservable.subscribe(x => console.log(x));\n```\n\n`Observable.subscribe`と`Observable.create()`の引数の関数名は同じである必要はないが,実用上概念的には同じとみなすことができる.\n\n`Observable.subscribe`は\"Observable execution\"を開始し,実行における**一つのObserver**に対し値やイベントを送信するシンプルな方法の一つ.\n\n`Observable.subscribe`は`addEventListener`や`removeEventListener`のようなevent handler APIとは根本的に異なる.\n\n### Executing Observables\n\nObservable Executionによって送信することができる値のタイプが3種類ある.\n\n- \"Next\" notification: `Number`,`String`,`Object`のような値を送信する.\n- \"Error\" notification: JavaScript errorやexceptionを送信する.\n- \"Complete\" notification: 値を送信することはできない.\n\n\"Error\"と\"Complete\"はObservable Exection中に一度しか呼び出しは起こらない.そしてどちらかが呼び出された場合は,もう片方が呼び出されることはない.\n\n> 一度のObservable Exectionにおいて,zeroから無限のNext notificationが送信される.もし,ErrorかComplete notificationが送信されたら,それ以降は何も送信されない.\n\nsee also\n\n- observable-contract.js\n\n### Disposing Observable Executions\n\nObservable Executionは無限になりうるし,有限時間内にObserverがExecutionを破棄したいようなケースもある.これを満たすために,cancellation APIが必要になる.\n\n一つのExecutionにつきただひとつのObserverに対して排他的なので,いったんObserverが値を受け取り終えたら,無駄な計算やメモリリソースを避けるためにExectionを止める手段を持っている必要がある.\n\n`Observable.subscribe`はObservable executionインスタンスが新たに生成される.これをSubscriptionと呼ぶ.\n\n```js\nconst subscription = observable.subscribe(x => console.log(x));\n```\n\nSubscriptionは実行中のExecutionを表し,Executionをキャンセルするための最小限のAPIを提供する.\n\n`Subscription.unsubscribe()`で実行中のExecutionをキャンセルすることができる.\n\n```js\nconst observable = Rx.Observable.from([10, 20, 30]);\nconst subscription = observable.subscribe(x => console.log(x));\nsubscription.unsubscribe();\n```\n\n`Observable.create()`を使ってObservableインスタンスを生成するときは,すべてのObservableは実行中のリソースをどう破棄するかを定義する必要がある.(must)\n\nsee also\n\n- define-dispose.js\n\n---\n# Observer\n\n`Observer`は`Observable`から送信された値に対するConsumer.\n\nObserverは\n\n- `next`\n- `error`\n- `complete`\n\nObservableから送信された各typeの値のcallbackを持つObject.部分的な定義も可能.\n\n```js\nconst observer = {\n  next: x => console.log(`Observer got a next value: ${x}`),\n  error: err => console.error(`Observer got an error: ${err}`),\n  complete: () => console.log(`Observer got a complete notification`),\n};\n```\n\n仮に,`Observable.subscribe()`の引数にObserverとしてのObjectではなく,functionを渡した場合は,subscribe内で引数で受け取ったfunctionをnext notificationを受け取るcallbackとしてobserver Objectを生成する.また,3つのtypeに対するcallbackをすべて関数として渡すこともできる.\n\n---\n# Subscription\n\n`Subscription`は\n\n- 破棄可能なリソースで通常は実行中のObservableを表現するObject.\n- Subscriptionが保持するリソースの解放や,実行中のObservableをキャンセルするための`unsubscribe` function(引数を取らない)を持つ.\n\n```js\nconst observable = Rx.Observable.interval(1000);\nconst subscription = observable.subscribe(x => console.log(x));\n// later\nsubscription.unsubscribe();\n```\n\nリソースを明示的に解放する必要がある場合は,`Observable.create`に渡すsubscribe functionの戻り値にリソース解放のための関数を返すようにする.\n\nsee also\n\n- define-dispose.js\n\nsubscriptionは親子関係を定義することもできる.\n\n```js\nconst subscription = observable1.subscribe(x => console.log(x));\nconst childSubscription = observable2.subscribe(x => console.log(x));\n\nsubscription.add(childSubscription);\n\nsetTimeout(() => {\n    // unsubscribe parent and child subscription\n    subscription.unsubscribe();\n}, 1000);\n```\n\nsee also\n\n- unsubscribe-multi-subscription.js\n\n`Subscription.add(otherSubscription)`で定義した関係をundoするために`Subscription.remove(otherSubscription)`がある.\n\n---\n# Subject\n\n`Subject`は特別なtypeの`Observable`.\n\n- 複数のObserverに対し値を送信できる.\n- EventEmitterのようなもので,複数のlistenerを管理する.\n- `Subject.subscribe`は`Observable.subscribe`とは異なり,値を送信する新しい実行呼び出しではなく,Observer listに新たなObserverを登録するのみ.(`addListener`のようなもの.)\n- すべてのSubjectはObservableでもあり,Observerでもある.\n  - `next(v)`,`error(e)`,`complete()`メソッドを持つ.\n  - Subjectに対し新しい値を与えるには,`next(theValue)`メソッドを呼ぶ.これにより,SubjectをlistenしているすべてのObserverに対しマルチキャストされる.\n\nマルチキャストの例\n\nsee also\n\n- multicast-to-observers.js\n\nSubjectはObserverでもあるので`Observable.subscribe`の引数として渡すこともできる.\nこれにより,ひとつのObserverへのキャストから複数のObserverに対してSubjectを介してマルチキャストすることへの変更も可能となる.\n\nSubject typeには,より特化した`BehaviorSubject`,`ReplaySubject`,`AsyncSubject`がある.\n\n## Multicasted Observables\n\nObservable, Subject, 複数のObserversによるマルチキャストの仕組みは以下の通り.\n\n- 複数のObserverはSubjectに対しsubscribeする.\n- SubjectはsourceとなるObservableをsubscribeする.\n\nマルチキャストを実現する方法は2つある.\n\n- `Observable.subscribe(subject)`を使う.\n- `Observable.subscribe(subject)`を内部的に実行する`ConnectableObservable.connect()`を使う.\n\n`multicast()`はsubscribeされたときにSubjectのように複数のObserverにキャストする振舞いを行う`ConnectableObservable`を返す.\n\n`ConnectableObservable.connect()`は`Subscription`を返すのでshared Observable executionをキャンセルするために`Subscription.unsubscribe`を呼び出す.\n\nsee also\n\n- multicasted-observable.js\n\n### Reference counting\n\n`ConnectableObservable.connect()`を明示的に呼び出して,Subscriptionをハンドリングするのは扱いづらいケースもある.最初のObserverが到達したら自動的にconnectされ,最後のObserverがunsubscribeしたらshared executionを自動的にキャンセルしたいというのが一般的.\n\n`Observable.refCount()`を使うと,上記の要件を満たす実装が可能.\n\nsee also\n\n- refcount.js\n\n---\n\n## BehaviorSubject\n\n\"the current value\"という概念をもつSubject.Consumerにemitした値を\"the current value\"として保持し,新しいObserverが増えた時は即座に\"the current value\"をObserverは受け取ることができる.\n\nsee also\n\n- behavior-subject.js\n\n## ReplaySubject\n\nold valuesとしてObservable executionの一部を保持しておくことができる.\n\n- buffer sizeで固定長の値を保持.\n- window timeをミリ秒単位で保持時間を指定する.\n\nsee also\n\n- window-time-replay-subject.js\n\n## AsyncSubject\n\n`AsyncSubject`はObservable executionの最後の値のみが,`AysncSubject.complete()`が呼ばれた時に送信される.\n\nAsyncSubjectは`last()` operatorに似ている.\n\nsee also\n\n- async-subject.js\n\n---\n\n# Operators\n\nRxJSで核となるのはObservableだが,operatorsのおかげで複雑な非同期処理もコンポーザブルで宣言的な書き方が可能になる.\n\n## What are opeartors?\n\noperatorsはObservableのメソッド(map, filter, merge, etc).呼び出されるとObservableインスタンスに副作用を与えるのではなく新しいObservableインスタンスが返される(pure function).\n\n基本的な考え方は,inputとしてObservableを受け取り,inputをSubscribeする新たなObservableを生成してoutputとして返す.\n\nsee also\n\n- operators-example.js\n\n## Instance operators versus static operators\n\ninstance methodはわかりやすい.\n\n```js\nRx.Observable.prototype.multiplyByTen = function multiplyByTen() {\n  const input = this;\n  return Rx.Observable.create(function subscribe(observer) {\n    input.subscribe({\n      next: v => observer.next(10 * v),\n      error: err => observer.error(err),\n      complete: () => observer.complete()\n    });\n  });\n}\n\nconst observable = Rx.Observable.from([1, 2, 3, 4]).multiplyByTen();\nobservable.subscribe(x => console.log(x));\n```\n\nstatic methodはObservable classにアタッチされるpure functionのこと.例としては,`interval`のようにミリ秒ごとに値をemitするようなものが挙げられる.\n\n```js\nvar observable = Rx.Observable.interval(1000);\n```\n\n`interval`や`create`のようなObservableを生成するようなメソッドが異なる性質を持つstatic methodもある.`merge`や`combineLatest`,`concat`のように引数として複数のObservableをとるようなメソッドもある.\n\n## Marble diagrams\n\nObservableの実行系列やoperatorsによるinput/outputの変化等を見るためにはmarble diagramsが表現しやすい.\n\nsee also:\n\nhttp://reactivex.io/rxjs/manual/asset/marble-diagram-anatomy.svg\n\n## Choose an operator\n\nどのoperatorsを使えばよいかを探す方法として以下を使うと良い.\n\n- http://reactivex.io/rxjs/manual/overview.html#choose-an-operator\n\n---\n# Scheduler\n\nSchedulerはsubscriptionがいつ開始するか,通知がいつ送信されるかを管理するもの.\n\n次の3のコンポーネントから成る.\n\n- data structure: 優先度や他の基準を基にしてどのようにtaskを保持したりキューイングしたりするかを責務とする.\n- execution context: いつどこでtaskが実行されるかを示す.(e.g.即時実行,setTimeoutやprocess.nextTickやanimation frameといったコールバックにおける実行)\n- (virtual) clock: `now()`によって\"time\"の概念を提供する.特定のSchedulerによってスケジューリングされたtaskはclockで示されるtimeにのみ従って実行される.\n\nSchedulerはObservableがObserverに対しどのような実行コンテキストで通知を送信するかを定義するためのもの.\n\nサンプルでは,1,2,3という値を同期的にひとつのObservableがemitし,emitする値を送信するための`async` schedulerを指定している.\n\nsee also\n\n- how-do-you-define-execution-context.js\n\n`Rx.Scheduler.async.schedule()`メソッドを使用することでSchedulerが内部にもつclockで管理される時間によってdelayを実現することもできる.\n\n## Scheduler Types\n\n|Scheduler | Purpose |\n|:---|:---|\n|null|scheduleｒを何も指定しなければ,通知は同期的かつ再帰的に送信される.固定時間の操作や再帰的な操作に使用する.|\n|Rx.Scheduler.queue|現在のイベントフレームにおけるqueueをスケジューリングする.繰り返し操作のために使用する.|\n|Rx.Scheduler.asap|とても小さなtask queueをスケジューリングする.Node.jsのprocess.nextTick()やWeb Worker MessageChannelやsetTimeout等の利用可能な最速の送信メカニズムを使用する.非同期での対話に使用する.|\n|Rx.Scheduler.async|setIntervalを使ってスケジュールが動作する.時間を指定したいような操作で使用する.|\n\n## Using Schedulers\n\nSchedulerとしてどのタイプのもの使うかを明示的に指定することなくSchedulerを使用することもできる.\n\n並列処理を扱えるすべてのObservable operatorsはoptionalなSchedulerを持っている.もしSchedulerを指定しなければRxJSは最低並列処理の原則に従ってデフォルトのSchedulerを選択する.これにより使用されるoperatorの要件を満たす最小の並列度をSchedulerは選択するということを意味する.例えば大量で無限のメッセージを返すようなoperatorに対してはqueueタイプのSchedulerが使用され,時間を使用するようなoperatorの場合にはasyncタイプのSchedulerが選択されたりする.\n\nパフォーマンスを最適化したい場合に,デフォルトとは異なるタイプのSchedulerを選択することができる.\n\nstatic creation operatorは`from(array, scheduler)`のように引数にSchedulerを取れることが一般的な設計になっている.\n\n`Observable.subscribe()`は同期的かつ即座にObservableが発生する.`Observable.subscribeOn()`の引数としてschedulerを指定するとどんなコンテキストでsubscribeするかをスケジューリングすることができる.\n\n`Observable.observeOn()`はどのようなコンテキストで通知が送信されるかをスケジューリングすることができる.source Observableとdestination Observerの間のmediatorが指定されたSchedulerを使ってdestination Observerをコールする.\n\n`bufferTime`や`debounceTime`,`throttleTime`,`timeInterval`といった時間に関連するObservable instance methodも引数にSchedulerタイプを指定することができる.デフォルトは`Rx.Observable.async`となっている.\n\nReplaySubjectも時間を扱うのでSchedulerを指定することができる.デフォルトは与えられたclockに対してqueue Schedulerを試用する.\n"
                                      }
                                    }
                                  ]
                                }
                              }
                            ]
                          }
                        }
                      ]
                    }
                  },
                  {
                    "type": "tree",
                    "name": "2017",
                    "object": {
                      "entries": [
                        {
                          "type": "tree",
                          "name": "01",
                          "object": {
                            "entries": [
                              {
                                "type": "tree",
                                "name": "02",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "react_rxjs_using_recompose.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/d35d638dc7d9ce4b27cb1d88638ce2b41251cb19",
                                        "text": "React RxJS using recompose\n===\n\n[RxJS](https://github.com/ReactiveX/rxjs)をReactと組わせる際にどういうふうにやるかについて調べた時のメモ.\nいくつか方法はあるみたいだけどどのように組み合わせるかを知りたかったのでミニマルにできそうな[recompose](https://github.com/acdlite/recompose/)をこのエントリではとりあげる.\n\nRxJSのmanualにもrx-react-componentを利用したサンプルがある.\n\n- http://reactivex.io/rxjs/manual/tutorial.html#react\n\n### 前提\n\n- node v6.9.0\n- RxJS v5.0.2\n- react v15.4.1\n- recompose v0.21.2\n\n### 対象読者\n\n- ReactやRxJSについては特に説明しないので,そのあたりの知識がある人\n\n### recomposeについて\n\nHigher-order Components(HoCs)と呼ばれる,React Component(Base Component)を受け取り,新しいReact Component(Enhanced Component)を返す関数を集めたライブラリがrecomposeである.\n\n```js\nconst EnhancedComponent = hoc(BaseComponent)\n```\n\nHoCsの使用例としては,\n\nreact-reduxの`connect()`関数が挙げられる.connect関数でWrapされたReact Componentは`SomeComponent.contextTypes`に親から受け取りたい値の定義が追加され,[React Context](https://facebook.github.io/react/docs/context.html)の仕組みが適用されて,親Componentから指定した値が暗黙的にpropsとして渡されるといったことを実現することができる.\n\nreact-reduxのconnect()関数とProvider Componentを利用することによってstateの変化をSubscribeする責務をライブラリ側が担ってくれるというメリットを享受することができる.\n\nHoCsの別の利用例としては,レンダリングパフォーマンスチューニングをする例も挙げられる.`pure()`というHoCで,React ComponentのLifeCycleメソッドの一つである`shouldComponentUpdate(nextProps)`をpropsのshallowEqualsで判断するような拡張を行う.\n\n### recomposeを使ったReactとRxJSの組み合わせ\n\nrecomposeは[Observable utilities](https://github.com/acdlite/recompose/blob/c7efea4dd5de14975d7ae32cd37396eec72c087e/docs/API.md#observable-utilities)として,ReactとStreamライブラリを組み合わせるAPI群を提供している.\n\n- `createEventHandler()`\n- `componentFromStream()`\n- `mapPropsStream()`\n\n#### `setObservableConfig()`\n\nObservable utilitiesはデフォルトでは[ES Observable proposal](https://github.com/tc39/proposal-observable)の実装Observableを使うようになっている.RxJSのObservableをStreamとして使用するために設定を行う必要がある.\n\n```js\n'use strict';\n\nconst Rx = require('rxjs');\nconst {setObservableConfig} = require('recompose');\nsetObservableConfig({\n  fromESObservable: Rx.Observable.from\n});\n```\n\n設定プロパティは`fromESObservable`の他にも`toESObservable`もあるが,今回使うことはなかったし,定義しなければ引数を受け取ってそのまま引数を返す関数としてデフォルトの設定になっているの何もしていない.\n\n#### `createEventHandler()`\n\n`createEventHandler()`はhandler(publisher)とhandlerがpublishする値をStreamとして扱うStream(Observable)のPairが提供される.\n\n```js\n'use strict';\n\nconst React = require('react');\nconst {Component} = React;\nconst {createEventhandler} = require('recompose');\n\nconst Counter = ({count = 0, handleIncrement}) => {\n  return (\n    <div>\n      Count: {count}\n      <button type=\"button\" onClick={handleIncrement}>+</button>\n    <div>\n  );\n};\n\nclass App extends Component {\n  constructor(props) {\n    super(props);\n    this.state = {count: 0};\n  }\n\n  componentDidMount() {\n    const {handler, stream} = creatEventHandler();\n    this.incrementSubscription = stream.mapTo(1).startWith(0).subscribe({\n      next: v => {\n        const count = this.state.count + v;\n        const nextState = Object.assign({}, this.state, {count});\n        this.setState(nextState);\n      }\n    });\n    this.handleIncrement = handler;\n  }\n\n  componentWillUnmount() {\n    if (this.incrementSubscription) {\n      incrementSubscription.unsubscribe();\n    }\n  }\n\n  render() {\n    const {count} = this.state;\n    const counterProps = {count, handleIncrement: this.handleIncrement};\n    return (\n      <Counter {...counterProps} />\n    );\n  }\n}\n\nmodule.exports = App;\n```\n\n適切なReact Lifecycleメソッド内でのStream関連のリソース解放をしている.Streamのリソース解放やその他のReact Lifecycleメソッドにおけるpropsのsubscribe等の責務を抽象化したHoCが`componentFromStream()`にあたる.\n\n#### `componentFromStream()`\n\n```js\n'use strict';\n\nconst Rx = require('rxjs');\nconst {createEventHandler, componentFromStream} = require('recompose');\n\nconst Counter = require('./path/2/counter.js');\n\nconst {stream, handler: handleIncrement} = createEventHandler();\nconst increment$ = stream.mapTo(1).startWith(0);\n\nconst count$ = increment$\n  .scan((count, diff) => count + diff, 0);\n\nconst EnhancedComponent = componentFromStream(props$ => {\n  return props$.combineLatest(\n    count$,\n    (props, count) => {\n      const nextProps = {...props, count, handleIncrement};\n      return (\n        <Counter {...nextProps} />\n      );\n    }\n  );\n});\n\nmodule.exports = EnhancedComponent;\n```\n\n`componentFromStream()`には,Componentに渡ってくるpropsのStream(`props$`)を受け取り,React ElementのStreamを返す関数(recompose内ではpropsToVdomと表現されている)を引数として渡す.\n\n`componentFromStream()`が行う処理は,\n\n- `props`のStream(`props$`)とこのStreamの値をpublish(emit)するPublisher(`propsEmitter`)を生成し,「propsのStreamを受け取ってReact ElementのStreamを返す関数(`propsToVdom`)」に`props$`を渡してReact ElementのStream(`vdom$`)を生成する.\n- React Lifecycleメソッド`componentWillMount`内で`vdom$`のsubscribeを開始し,Subscriptionを生成する.subscribe関数ではpublishが呼ばれるたびにReact Elementが渡ってくるのでその値を`setState()`する.また,initial propsを`propsEmitter`でpublish(emit)する.\n- React Lifecycleメソッド`componentWillReceiveProps(nextProps)`内でpropsを受け取ったタイミングでpropsを`propsEmitter`でpublish(emit)する.\n- React Lifecycleメソッド`componentWillUnmount()`内でSubscriptionをunsubscribeする.\n- `render()`メソッドではstateのReact Elementを描画する.\n\n#### `mapPropsStream()`\n\nこれは内部的には,`componentFromStream()`を使っている.\n\n```js\n'use strict';\n\nconst {createEventHandler, mapPropsStream} = require('recompose');\n\nconst Counter = require('./path/2/counter.js');\n\nconst {stream, handler: handleIncrement} = createEventHandler();\nconst increment$ = stream.mapTo(1).startWith(0);\n\nconst count$ = increment$\n  .scan((count, diff) => count + diff, 0);\n\nconst withCount = mapPropsStream(props$ => {\n  return props$.combineLatest(\n    count$,\n    (props, count) => {\n      const nextProps = {...props, count, handleIncrement};\n      return {...nextProps};\n    }\n  );\n};\n\nconst EnhancedCounter = withCount(props => {\n  return <Counter {...props} />\n});\n\nmodule.exports = EnhancedCounter;\n```\n\n`mapPropsStream()`には「受け取るpropsのStream(`props$`)を受け取り,新たなpropsのStreamを返す関数」を渡す.これで「propsを受け取ってReact Elementを返す関数」(例では`withCount()`)が生成される.\n\n## まとめ\n\n- 小さめのサンプルでReactとRxJSの組み合わせの仕組みを見た.\n- recomposeのObservable utilitiesを使うことで,Streamのリソース解放や適切なタイミングでのpublish等を気にする必要がなくなる.\n- React SFCとHoCによるコンポーネント化の具体例を見た.\n\n\nある程度複雑なアプリじゃないとRxJS使うメリットは無いけど,Reactとの組み合わせ方は把握できたので,もっと複雑な例で素振りしてみたい.\n"
                                      }
                                    }
                                  ]
                                }
                              },
                              {
                                "type": "tree",
                                "name": "03",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "exploring_ecmascript_decorators.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/380806067666ab84f28263347ae297768970fc21",
                                        "text": "Exploring EcmaScript Decoratorsを読んだときのメモ\n===\n\n### materials\n\n- Exploring EcmaScript Decorators\n  - https://medium.com/google-developers/exploring-es7-decorators-76ecb65fb841#.3ercj63kr\n- tc39/proposal-decorators\n  - https://github.com/tc39/proposal-decorators\n\n## The Decorator Pattern\n\nPythonにおいては,decoratorsは高階関数(higer-order functions)を呼び出すためのシンプルなsyntaxを提供する.別の関数を引数に取る関数で,元の関数の振舞いを明示的に修正することなしに拡張することができる.\n\nDecoratorsの使用例としては,\n\n- memoizeによる関数実行結果のキャッシュによる高速化\n- アクセスコントロールと認証の強制\n- 計測関数\n- ロギング\n- 律速\n\nなどが挙げられる.\n\n## Decorators in ES\n\n提案中のdecoratorsはclass,properties,object literalが対象とされている.\n\n### Decorating a property\n\nsee also\n\n- readonly-decorators.js\n\ndecorator libraryとしてcore-decorators.jsというのもある.\n\n- https://github.com/jayphelps/core-decorators.js\n\n### Decorating a class\n\n現在策定中の仕様によると,classのdecoratorはconstructorを受け取る.\n\nsee also\n\n- class-decorator.js\n\n### ES2016 Decorators and Mixins\n\n詳しくは以下の記事を参照のこと.\n\n- Using ES.later Decorators as Mixins\n  - http://raganwald.com/2015/06/26/decorators-in-es7.html\n- Functional Mixins in ECMAScript 2015\n  - http://raganwald.com/2015/06/17/functional-mixins.html\n\nmixin関数を定義してdecoratorsを使ってclassに振舞いを追加することができる.\n\nsee also\n\n- decoratos-as-mixins.js\n\n## Conclusion\n\ndecoratorsは以下のメソッドを使って実現するメタプログラミングのように感じた.\n\n- [Object.getOwnPropertyDecriptor()](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/getOwnPropertyDescriptor)\n- [Object.defineProperty()](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/defineProperty)\n- [Reflect.ownKeys()](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Reflect/ownKeys)\n\n#### Appendix\n\n- sample example\n  - https://github.com/rkaneko/learn-es-decorators\n"
                                      }
                                    }
                                  ]
                                }
                              },
                              {
                                "type": "tree",
                                "name": "04",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "learn_redux-observable.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/e90187babecc5abc02ea81beed9fbe2525fd42d7",
                                        "text": "Learn redux-observable\n===\n\n### materials\n\n- https://redux-observable.js.org/\n\n# Epics\n\nEpicはredux-observableにおいて根本となる概念.\n\nactionのStreamを受け取って,actionのStreamを返す関数.\n\n```js\nfunction (action$: Observable<Action>, store: Store): Observable<Action>;\n```\n\n# Examples\n\n## Invoking Web APIs asynchronously\n\n- Web API module\n\n```js\n'use strict';\n\nrequire('isomorphic-fetch');\n\nfunction fetchUsers() {\n  return fetch(`/apis/users`)\n    .then(response => response.json());\n}\n\nmodule.exports = {\n  fetchUsers\n};\n```\n\n- Web API epic module\n\n```js\n'use strict';\n\n// You should import RxJS in an entry file for using RxJS operators\nconst Rx = require('rxjs');\n\nconst actionTypes = require('./path/2/action-types.js');\nconst webApi = require('./path/2/web-api.js');\n\nconst fetchUsersEpic = action$ => (\n  action$.ofType(actionTypes.FETCH_USERS)\n    .do(action => console.log(action))  // debug\n    .mergeMap(action => Rx.Observable.fromPromise(webApi.fetchUsers()))\n    .map(json => {\n        const {users} = json;\n        return {\n          type: actionTypes.RECEIVE_USERS,\n          payload: {users}\n        };\n    })\n    .do(action => console.log(action))  // debug\n);\n\nmodule.exports = {\n  fetchUsersEpic\n};\n```\n\n- redux reducer module\n\n```js\n'use strict';\n\nconst actionTypes = require('./path/2/action-types.js');\n\nconst initialState = {\n  users: []\n};\n\nfunction reducer(state = initialState, action) {\n  const {type, payload} = action;\n  switch (type) {\n  case actionTypes.RECEIVE_USERS:\n    const {users} = payload;\n    return {...state, users};\n  default:\n    return state;\n  }\n}\n\nmodule.exports = reducer;\n```\n\n- redux store module\n\n```js\n'use strict';\n\nconst {applyMiddleware, createStore} = require('redux');\nconst {combineEpics, createEpicMiddleware} = require('redux-observable');\n\nconst reducer = require('./path/2/reducer.js');\n\nconst {fetchUsersEpic} = require('./path/2/epic.js');\n\nconst rootEpic = combineEpics(\n  fetchUsersEpic\n);\nconst epicMiddleware = createEpicMiddleware(rootEpic);\n\nconst store = applyMiddleware(\n  epicMiddleware\n)(createStore)(reducer);\n\nmodule.exports = store;\n```\n\nあとはViewからactionをdispatchするhandlerが呼ばれると,epicMiddlewareによってepicが処理される.\n\n注意したいのは,actionがdispatchされると当然reducerにもactionが伝わるので,該当actionの処理を書いていれば当然その処理が走るし,書いていないければdefaultの処理が走る.\n\n> REMEMBER: When an Epic receives an action, it has already been run through your reducers and the state updated.\n\nEpic内で`store.dispatch()`を呼ぶのは控えたほうが良いみたい.anti-patternsと考えられていて将来的にはAPIからなくなる可能性あり.\n\n> Using store.dispatch() inside your Epic is a handy escape hatch for quick hacks, but use it sparingly. It's considered an anti-pattern and we may remove it from future releases.\n\n# Accessing the Store's State\n\nepic関数の中で同期的にstoreのcurrent stateにアクセスすることもできる.\n\n```js\nconst incrementIfOddEpic = (action$, store) => (\n  action$.ofType(INCREMENT_IF_ODD)\n    .filter(() => store.getState().counter % 2 === 1)\n    .map(() => {type: INCREMENT});\n```\n\n# Combining Epics\n\nredux-observableのcombineEpics関数はObservable.merge(epic...)のWrapper.\n\n# Cancellation\n\n例えば,`Rx.Observable.takeUntil()`とactionをdispatchすることを組み合わせて以下のようにもできる.\n\n```js\nconst fetchUsersEpic = action$ =>\n  action$.ofType(actionTypes.FETCH_USERS)\n    .mergeMap(action =>\n      Rx.Observable.fromPromise(webApi.fetchUsers())\n        .map(json => {\n          const {users} = json;\n          return {\n            type: actionTypes.RECEIVE_USERS,\n            payload: {users}\n          };\n        })\n        .takeUntil(action$.ofType(actionTypes.FETCH_USER_CANCELLED))\n    );\n```\n\nポイントとしては,メインのEpicを`takeUntil`するのではなく,ajaxコールのStreamを`takeUntil`しているところ.Epic自体をキャンセルしたいのではなく,非同期処理のみをキャンセルしたいというユースケースを想定している.\n\n# Error Handling\n\nエラー処理もEpicの主要件である.もっともシンプルな例は,epicの中でcatchしてerror情報を含むactionをemitすること.\n\n```js\nconst fetchUsersEpic = action$ =>\n  action$.ofType(actionTypes.FETCH_USERS)\n    .mergeMap(action =>\n      Rx.Observable.fromPromise(webApi.fetchUsers())\n        .map(json => {\n          const {users} = json;\n          return {\n            type: actionTypes.RECEIVE_USERS,\n            payload: {users}\n          };\n        })\n        .catch(err => Observable.of({\n          type: actionTypes.FETCH_USERS_REJECTED,\n          payload: {error},\n          error: true\n        }))\n    );\n```\n\n注意点は,`action$.ofType()`のStreamに`.catch()`を書いてしまうと,一度catchが呼ばれるとそのepicは二度と新たなactionをlistenしなくなってしまう.これはRxJSの`error: err => doSomething()`が一度しか呼ばれないという仕様に呼応している.\n\n# Writing Tests\n\n## Epic with invoking asynchronously Web APIs\n\n非同期APIリクエストを行うEpicのテストのやり方を考えてみる.\n\nテスト対象モジュールは,\n\n```js\nconst fetchUsersEpic = action$ => (\n  action$.ofType(actionTypes.FETCH_USERS)\n    .do(action => console.log(action))  // debug\n    .mergeMap(action => Rx.Observable.fromPromise(webApi.fetchUsers()))\n    .map(json => {\n        const {users} = json;\n        return {\n          type: actionTypes.RECEIVE_USERS,\n          payload: {users}\n        };\n    })\n    .do(action => console.log(action))  // debug\n);\n```\n\nテスティングライブラリは次のような構成.\n\n- [ava](https://github.com/avajs/ava)\n  - test runner\n  - assertion lib (power-assert)\n  - support async/await, Observable test\n- [fetch-mock](https://github.com/wheresrhys/fetch-mock)\n  - fetch (isomorphic-fetch) APIのmock\n- [redux-mock-store](https://github.com/arnaudbenard/redux-mock-store)\n  - redux storeのmock\n\n```js\n'use strict';\n\nconst test = require('test');\nconst fetchMock = require('fetch-mock');\nconst configureMockStore = require('redux-mock-store').default;\n\nconst Rx = require('rxjs');\nconst [combineEpics, createEpicMiddleware} = require('redux-observable');\n\nconst actionTypes = require('./path/2/action-types.js');\nconst {fetchUsersEpic} = require('./path/2/epic.js');\n\nlet epicMiddleware;\nlet rootEpic;\nlet mockStore;\nlet store;\ntest.before(t => {\n  rootEpic = combineEpics(\n    fetchUsersEpic\n  );\n  epicMiddleware = createEpicMiddleware(rootEpic);\n  mockStore = configureMockStore([epicMiddleware]);\n});\n\ntest.beforeEach(t => {\n  store = mockStore();\n});\n\ntest.afterEach(t => {\n  fetchMock.restore();\n  epicMiddleware.replaceEpic(rootEpic);\n});\n\ntest('If you dispatch action: FETCH_USERS then dispatch action: RECEIVE_USERS asynchronously', t => {\n  // setup\n  const responseJson = {\n    users: [\n      {\n        name: 'Bob',\n        age: 25\n      }\n    ]\n  };\n  fetchMock.get(/apis\\/users/, {\n    body: responseJson\n  });\n\n  // exec\n  store.dispatch({\n    type: actionTypes.FETCH_USERS\n  });\n\n  // verify\n  t.truthy(fetchMock.lastCall(/apis\\/users/));\n  return Rx.Observable.interval(500)\n    .take(4)\n    .mapTo(store.getActions())\n    .first(actualActions => actualActions.length === 2)\n    .map(actualActions => {\n      return t.deepEqual(\n        actualActions,\n        [\n          {\n            type: actionTypes.FETCH_USERS\n          },\n          {\n            type: actionTypes.RECEIVE_USERS,\n            payload: {\n              users: responseJson.users\n            }\n          }\n        ]\n      );\n    });\n});\n```\n\nObsrevable使わないで以下のように書くと,storeにdisapatchされたactionが`FETCH_USERS`のみになっていることがあって,非同期されるdispatchされる`RECEIVE_USERS`がassert時にはまだstoreにdispatchされていないことがあってテストが失敗する.\n\n```js\n// verify\nt.truthy(fetchMock.lastCall(/apis\\/users/));\nt.deepEqual(\n  store.getActions(),\n  [\n    {\n      type: actionTypes.FETCH_USERS\n    },\n    {\n      type: actionTypes.RECEIVE_USERS,\n      payload: {\n        users: responseJson.users\n      }\n    }\n  ]\n);\n```\n\n#### Appendix\n\n- https://github.com/rkaneko/react-examples/tree/master/redux-observable-example\n"
                                      }
                                    },
                                    {
                                      "type": "blob",
                                      "name": "notes_dot_net_eaa.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/a60650ba3ad8e88b9582a9a52c8187d000f3d64a",
                                        "text": ".NET EAAを読んで考えたこと\n===\n\n# 第3部 サポートアーキテクチャ\n\n## 第8章 ドメインモデルの紹介\n\n- すべては振る舞いのために\n\nドメインモデルの目的は,アプリケーションで扱われる基本的なビジネスコンセプトをできるだけ忠実に表現することにある.\n\nドメインモデルに課される制約の目的は,クラスを基本的なビジネスコンセプトと完全に一致させ,常に整合性を確保することにある.\n\nこのために,データよりも振る舞いに重点を置いた設計にする必要がある.\n\n- DDDはいつでも誰にとっても効果的\n\n分析とは,ドメインの仕組みを理解し,それを適切な語彙(ユビキタス言語)で表現すること.\n\nDDDの分析が効果を得やすいのは長期的なプロジェクト.見返りが期待出来なさそうなら,境界づけられたコンテキストごとに別のサポートアーキテクチャを選択する.\n\n- ドメインモデルは永続化に関知しない\n\nドメインモデルに永続化レイヤーへのアクセスを要求するメソッドを与えるべきではない.\n\n理想はまったくインフラレイヤーに依存しないドメインモデルだが,フレームワーク制約を受けることはままあるので,依存関係を最低限にしたドメインモデルの構築が現実的な目標になりうる.\n\n- ドメイン層の内側\n\nドメイン層の大雑把な構成は,\n\n- ドメインモデル(エンティティ,値オブジェクト)\n- ドメインサービス(ここでいうサービスはビジネスロジックのことで,SOAのサービスやWebサービスのそれとは異なる)\n- モジュール\n\nエンティティの代わりに永続化を担うのがリポジトリ.\n\n△リポジトリがエンティティの永続化を担う\n○リポジトリがエンティティのサブセットである集約の永続化を担う\n\n- 集約\n\nドメインモデルを扱っていると常に同時に使用/参照されるエンティティの存在に気づく.\n\n集約が担うのは,モデル内のエンティティをまとめたり区切ったりする整合性の境界を決めること.\n整合性はたいていトランザクションに紐づく.\n\n集約はビジネスの整合性を担保するための手段.\n集約はドメインモデルの不変条件を持つ.\n\nルートエンティティは集約に含まれるエンティティをカプセル化し,それらのプロキシとして機能する.\n\nうまく設計された集約では１トランザクションにつき1つの集約が変更される.\n\n子エンティティにパブリックコンストラクタを設けると集約の整合性が失われる可能性があるため避けたい方が良い.\n\n集約のメリットは,モデルが複数の別のモデルと関連付けられて複雑なリレーションになるのを防ぎやすい考え方であるという点.\n\n集約におけるルール\n\n  - ルートエンティティが参照できるのは同じ集約内のエンティティか別の集約のルートエンティティのみ.\n    - 集約内のエンティティにはライフサイクルや同一性があるので集約の外から直接参照できない.(ルートエンティティ経由で参照する)\n  - 子エンティティは別のルートエンティティへの参照を保持できる.\n\n集約における不変条件の例\n\n商品発送後は注文情報を更新できないというビジネス・ルールが要求される場合には,集約ルートは注文情報をコードから更新できないようにしなければならない.\n\n集約にカプセル化されているオブジェクトの永続化の責務はすべて集約にある.\n\nデータベース開発者に対しての集約のイメージ\n\n  - 集約: 外部キー関係でリンクされたテーブルの和集合\n  - 集約ルート: 外部キー関係のグラフにおいて,外向きの外部キー参照のみを含んでいる一意なテーブル\n\n- ドメインサービス\n\nドメインロジックを実装するクラスで,特定の集約に属さない.\n\nビジネスアクションを実装する目的で,集約やリポジトリのアクティビティを調整する.\nメールやテキスト送信が必要な場合はインフラレイヤのサービスを使用することもある.\n\nドメインサービスの使用を検討するのは,ビジネスロジックがどの集約にも適合せず,処理に合わせて既存の集約設計を見直すことが不可能な場合.いわゆる最後の砦.\n\nドメインサービスが実装するアクションは要件にもとづいている必要がある.\nドメインサービスで使用される名前はユビキタス言語である必要がある.\nアプリケーションサービスから呼び出されるアクションを定義する.\n\nドメインサービスの例\n\nゴールドカスタマーは以下の条件を満たす顧客を言う.\n\n  - 指定された範囲の商品をしきい値を超えて発注した場合\n\nこれを判定するためには,データベースアクセスが必要になる.なのでエンティティはデータベースアクセスに無関心であるという制約のもと実装するためには顧客エンティティがIsGoldのようなメソッドを持つのは制約違反になる.\nなのでドメインサービスとして定義するのが良い.\n\n- リポジトリ\n\nルートエンティティごとに1つのリポジトリを持つ.\n\nインターフェースはドメインレイヤに,実装はインフラレイヤに持つ.DIPを使うことが多い.\n実装ではORMやデータベース・アクセスのライブラリ・フレームワークを使用する.\n\n- ドメインイベント\n\nドエインイベントを利用するメリットは,イベントを作成するコードに触れず,ハンドラー側にイベントごとの振る舞いを追加できる,複数の場所でイベントを発生させることができるようになるという点.\n\n余談として,私の経験ではドメインイベントを利用した方が実装には拡張性があると考えたが,ビジネス側からブラウザクッキーと結びつけが顧客登録時に欲しい,メールの開封率はとても低いのでその場でトラッキングIDが確定するとMAを運用する上で価値が出しやすいという要件をもらったのでシーケンシャルな実装をせざるを得なかったということがあった.\n\n- 横断的関心事\n\nインフラストラクチャ層は横断的関心事に対処する場所でもある.\n\n  - セキュリティAPI(認証/承認)\n  - ログ\n  - トレース\n\n- 検証\n\nドメインモデル,ドメインサービスで適切な粒度で不変条件,整合性チェックを行う.\n\n- セキュリティ\n\n承認の側面を考える.\n\nアプリケーション層では,許可された呼び出しでのみがドメインロジックにアクセスできるように制御すべき.\n\nセキュリティの粒度とユースケースが一致する場合は,プレゼンテーション層の入り口で承認を行うこともある.\n\n- ログ\n\nドメインモデルからログを出力することがビジネス上きわめて重要と判断される場合はDIPなどを使ってインフラレイヤーの実装を使ってログ出力する.\n\n- キャッシュ\n\n読取りのパフォーマンスを担保するためにキャッシュを利用することがある.\n実装はインフラレイヤーに隠す.\n\n私の聞いた話では,広告界隈ではレスポンス速度がビジネス上きわめて重要なので集約単位でキャッシュしたという事例がある.\n\n---\n\n## 第9章 ドメインモデルの実装\n\nI-By-Stuffとしてオンラインストアの例を取り上げてドメインモデルの設計実装例を説明している章.\n\n- ビジネスロジックの場所\n  - メソッドのコードがエンティティのメンバのみを扱う場合は,おそらくエンティティに属する.\n  - メソッドのコードが同じ集約内の他のエンティティまたは値オブジェクトにアクセスする必要がある場合は,おそらく集約ルートに属する.\n  - メソッドのコードが永続化レイヤに対する問い合わせや更新を必要とする場合,またはエンティティの境界の外側にある参照を取得する必要がある場合,ドメインサービスメソッドに属する.\n\n- DDDのエンティティの特徴\n\n  - 同一性が明確に定義されている.\n  - 振る舞いを持つ(public/非public).\n  - 状態が読み取り専用のプロパティによって表現される.\n  - プリミティブ型の使用は限定的で値オブジェクトと置き換えられる.\n  - 複数のコンストラクタよりもファクトリメソッドが優先される.\n\n- エンティティの同一性\n\n  - IDやその他の組み合わせ(名前と番号のような)で表現される.\n  - エンティティの同一性は同一性プロパティによって定義され,判定される.\n\n- 値オブジェクト\n\n  - エンティティとは異なり可変の状態を持たない.\n  - データによって識別されるため同一性を必要としない.(プロパティがすべて同じなら同じ値とみなす.)\n  - コンストラクタのみで値を受け取る.\n  - public getterを通じて値を提供.\n  - Immutable.\n  - hash/equalsメソッドを実装.\n  - プリミティブ型と置き換えて使用されることがしばしば.\n\n-　集約の設計\n\n  - コンストラクタではなく,ファクトリメソッドとしてインスタンス生成方法を提供することで,データにおける制約を定義したり,ユビキタス言語に沿った実装ができる.\n  - 集約内にコレクションを定義する場合はCQRSにおけるメリットも検討する?\n    - コレクションにおける書き込み・読み込みを同時に扱うことは複雑性が増しやすい.\n\n- ビューモデル\n\n  - ビューモデルは純粋なデータコンテナーであり,ドメインエンティティやDTO形式で表される.\n  - JSのreduxのstateはSerializableであることが求められることがしばしばで,この制約の元ではstateに振る舞いをもつObjectをもたせるのは厳しくなるので,データコンテナーとしてのViewModelと捉えるように求められていると考える.\n    - 本書ではViewModelがドメインエンティティをラッピングすることも紹介されているが,reduxにおいてはドメインモデルからViewModelを生成することになりそう.\n\n- ドメインイベント\n\n例えばオンラインストアで注文が作成されると,顧客のポイントカードにポイントを追加するといったことを行う場合,シーエンシャルに注文作成後の処理として実装することもできる.\n\nしかしながら,ビジネス上ポイント付与処理が頻繁に変更されるような要件のものであれば,シーケンシャルに処理するよりもイベントとハンドラによって疎結合に実装したほうが,テスタビリティがよくなったりや複雑度が抑えられる可能性がある.また別のイベントハンドラを定義することにより別の処理を行う拡張もしやすくなる.\n\n---\n"
                                      }
                                    }
                                  ]
                                }
                              },
                              {
                                "type": "tree",
                                "name": "13",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "my_thoughts_about_the_risk.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/764c9780f2caa65784fda4dc9256405eb522f774",
                                        "text": "リスクについて思うこと\n===\n\nリスクとは何か?リスクヘッジとは何をすることか?\n\nこのことについて考えるときに僕が影響を受けたは次の言葉.\n\n> Risk comes from not knowing what you're doing. \n>\n>    --- Warren Edward Buffett\n\n自分が何をしているのか知らないことからリスクは生まれる.\n\nこれを自分の文脈に当てはめて考えてみる.\n\n利用者に対し,価値を提供できなくなるリスクとはどういうことから生まれるのか?\n\n- 技術についてよく理解しないで利用する.\n- 技術は日々進歩するが,それをキャッチアップすることを怠り,より良い価値の創造の機会を損失する.\n- 利用者の利用状況を分析することなく,「どういうときに何をしたくてどうやって改善するか」を適当に決めたり,オーバーエンジニアリングする.\n\n学ぶことと価値を生み出すことへの時間配分のバランスは難しい.提供時期や競合関係といった要素も価値を生み出すことに影響を与えるからだ.\n\nこのバランスを考えるときには再び「自分が何をしているのか」について個人というスコープを超えて考える.\n\n例えばあるチームで「製品の価格体系の見直しをしたい.製品が提供する価値に対して納得感のある価格は,利用用途・会社規模・文化といった様々なものが影響する.なので柔軟な価格体系を提供できる販売システムが欲しい.」といった戦略が打ち出されたとする.\n\nこのときの要件例としては,価格体系変更・拡張にともなう画面の修正コストをさげてリリースをスピーディにできることが挙げられるだろう.\n\nもし自分がフロントエンドの設計・技術に対してよく知らずにこの要件を実現できないリスクがあると考えた場合には,\n\n- 自分で勉強する\n- より詳しい人に協力してもらう\n\nといった選択肢がある.\n\n戦略が会社にとってコアなもので利用者との価値交換の規模が大きいと判断される場合には,リスクヘッジのために適材適所へのリソースの再配置が必要になる.\n\nリソースの再配置は,よく知っている人を配置することの他にも,チームメンバーのできること・やりたいことを考慮してカバレッジを考えて割り当てるということも含まれる.\n\n学ぶことと価値を生み出すことのバランスを考えるのを個人単位で閉じるのではなく,チーム単位にまで拡張して考える.\n\n\n価値創造のためにチームワークとは何かについて本を読んだりする.そんななかで一番の根底にあるのは「リスクとは何か?」について考えることではないのかと現状の自分は思っている.\n"
                                      }
                                    }
                                  ]
                                }
                              },
                              {
                                "type": "tree",
                                "name": "16",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "learn_es2017_async_functions.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/ab69866ab8b7b65994b427d5e03be454e0144904",
                                        "text": "Ecmascript async functionsについて\n===\n\n### Materials\n\n- [Tips for using async functions](http://www.2ality.com/2016/10/async-function-tips.html)\n\n# 1. Promisesを知る\n\nasync functionsのベースとなっているのはPromisesである.\n特に,Promisesをベースとしていない既存コードとasync functionsを組み合わせるときには,Promisesを直接使う以外の選択肢はない.\n\nXHRをPromise化するにはcallback内でPromiseをresolve/rejectをする.\ncallback内ではfunctionの値をreturnしたりもthrowもできない.\n\nasync functionsのcommon coding styleは次のようになる.\n\n- 非同期なprimitivesを構築するためにPromisesを使う.\n- 非同期関数経由でそのprimitivesを使う.\n\n# 2. Async functionsは同期的に始まり,非同期に解決される\n\nasync functionsはどのように実行されるか?\n\n0. async functionsの結果は常にPromise.async functionsの実行開始時にこのPromise値は生成される.\n0. bodyが実行される.実行は`return`や`throw`を経由して永久的に終了する.もしくは`await`経由で一時的に終了する.この場合は通常後の処理が継続される.\n0. Promise値が返却される.\n\nsee also\n\n- src/async-func.js\n\n# 3. Promisesを返すものはwrapされない\n\nPromiseのresolveは通常の命令となる.\n\nasync functionsの中でPromise値を返しても,そのPromise値はPromiseでwrapされることはない.\n\nsee also\n\n- src/async-func-using-promise.js\n\nasync functionsを入れ子にした場合はどうなるか?\n\n次の２つのコードはだいたい同じようなイメージなる.ただし前者のほうがパフォーマンスが良い.\n\n```js\nasync function asyncAnotherFunc() {\n  return 5;\n}\n\nasync function asyncFunc() {\n  return asyncAnotherFunc();\n}\n\nasyncFunc()\n  .then(x => console.log(x));\n\n// output: 5\n```\n\n```js\nasync function asyncAnotherFunc() {\n  return 5;\n}\n\nasync function asyncFunc() {\n  return await asyncAnotherFunc();\n}\n\nasyncFunc()\n  .then(x => console.log(x));\n\n// output: 5\n```\n\nsee also\n\n- src/async-func-nested.js\n\n\n# 4. `await`を忘れては行けないケース\n\nasync function内でasync functionを呼び出す場合には`await`をつけないと意図した挙動にならない.\n\n```js\nasync function asyncFunc() {\n  // 関数の戻り値が欲しいが...\n  const value = otherAsyncFunc(); // missing await\n}\n```\n\nESLint [require-await](http://eslint.org/docs/rules/require-await)ではasync functionsで意図しない処理の値がasync functionsの戻り値になるのを防ぐために`await`を書くように強制できる.\n\n`await`はasync functionsが戻り値を返さない時にも意味をなす.呼び出し元に呼び出しが終了したことを伝えるシグナルとしてPromiseを使用することもできる.\n\nasync functionsのbodyでPromisesをawaitすると,awaitしているPromisesの処理が終わってから,async functionsが実行されることが保証される.\n\nsee also\n\n- src/await-promises.js\n\n## ちなみに\n\n実行エンジンでは,async functions内でPromisesをawaitしているasync functionsを呼び出す処理では,catchを書かないと怒られる.\n\n```\n(node:24554) UnhandledPromiseRejectionWarning: Unhandled promise rejection (rejection id: 2): TypeError: Promise resolver undefined is not a function\n```\n\n# 5. \"fire and forget\"を行う場合は`await`をつける必要はない\n\n非同期処理をトリガーするだけで終了には興味がないケースではawaitを書く必要がない.\n\n```js\nasync function asyncFunc() {\n  const writer = openFile('someFile.txt');\n  writer.write('hello');  // 書き込みはAPIが正しい順序で行うことを保証してくれている\n  writer.write('world');  // 書き込み終了のタイミングには関心がないのでawaitを書く必要はない\n  await writer.close();  // このasyncFunc()関数内でclose完了を保証したいので,await\n}\n\n```\n\n最終行の`await write.close();`では`asyncFunc()`が`writer.close();`が終了した後に実行が終了されることを保証している.\n\ncloseすることを保証することなく,returnすれば(Promisesでwrapされていない)Promisesを返すこともできる.\n\n```js\nasync function asyncFunc() {\n  const writer = opneFile('someFile.txt');\n  writer.write('hello');\n  writer.write('world');\n  return write.close();\n}\n```\n\nどちらのパターンもPros/Consがある.Dr.Axel的にはawait版のほうが理解しやすいとの意見.\n\n# 6. Parallelism\n\n次のコードは`asyncFunc1()`,`asyncFunc2`がシーケンシャルに実行される.\n\nsee also\n\n- src/async-func-sequential.js\n\n次のコードは`asyncFunc1`,`asyncFunc2`がパラレルで実行される.\n\nsee also\n\n- src/async-func-parallel.js\n\n\n#### Appendix\n\n- examples\n  - https://github.com/rkaneko/learn-es-async-functions\n"
                                      }
                                    }
                                  ]
                                }
                              }
                            ]
                          }
                        },
                        {
                          "type": "tree",
                          "name": "02",
                          "object": {
                            "entries": [
                              {
                                "type": "tree",
                                "name": "13",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "workaround-for-git-large-files-detected.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/95231af0d4611ea3e602c4ddf1f1c9850428b418",
                                        "text": "Workaround for Git Large files detected\n===\n\nGitHubに誤って100Mを超えるようなファイルをpushしてしまうと以下のようなエラーが出てpushに失敗する.\n\n```\nremote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\n```\n\nこのときは以下のように対処する.Large fileは`large-file.jar`とする.\n\n```bash\n$ git rm --cached path/2/large-file.jar\n\n$ git cm --amend -m \"delete too large files\"\n\n$ git filter-branch --force --index-filter \\\n  'git rm --cached --ignore-unmatch path/2/large-file.jar' \\\n    --prune-empty --tag-name-filter cat -- --all\n\n# push again\n$ git push origin some-branch-name\n```\n\npushしたい場合はlfsを検討する.\n"
                                      }
                                    }
                                  ]
                                }
                              },
                              {
                                "type": "tree",
                                "name": "19",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "setting-up-mysql-on-xenial.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/d969cdbef3cb29452f584251c1585acfb31a15d3",
                                        "text": "Setting up MySQL on xenial\n===\n\n久々にUbuntu 16.04 (xenial)にMySQLをinstallしたら,root userの利用に`sudo`が必要になっていて不便だったので`sudo`不要にする設定メモ.\n\n```bash\n# install\n$ sudo apt-get install mysql-server\n\n# connect mysql as a sudo user\n$ sudo mysql -uroot -p\n\nmysql> SELECT User, Host FROM mysql.user;\n+------------------+-----------+\n| User             | Host      |\n+------------------+-----------+\n| debian-sys-maint | localhost |\n| mysql.sys        | localhost |\n| root             | localhost |\n+------------------+-----------+\n3 rows in set (0.00 sec)\n\nmysql> DROP USER 'root'@'localhost';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> SELECT User, Host FROM mysql.user;\n+------------------+-----------+\n| User             | Host      |\n+------------------+-----------+\n| debian-sys-maint | localhost |\n| mysql.sys        | localhost |\n+------------------+-----------+\n2 rows in set (0.00 sec)\n\nmysql> CREATE USER 'root'@'%' IDENTIFIED BY '';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> GRANT ALL PRIVILEGES ON *.* TO 'root'@'%';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> FLUSH PRIVILEGES;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> exit\nBye\n```\n\nこれで`sudo`なしでMySQL consoleを起動できる.\n\n### Credits\n\n- http://askubuntu.com/questions/766334/cant-login-as-mysql-user-root-from-normal-user-account-in-ubuntu-16-04\n"
                                      }
                                    }
                                  ]
                                }
                              },
                              {
                                "type": "tree",
                                "name": "25",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "scala-matsuri-day1.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/55b0485e6a878b0555ec99e7b7fc8191cd9b6295",
                                        "text": "ScalaMatsuri Day1\n===\n\n# Akkaで分散システムの障害に備える\n\n- Slide\n  - https://www.slideshare.net/yugolf/preparing-for-distributed-system-failures-using-akka-scala-matsuri2017-72575226\n\n- Microservices = Distributed Sysytems\n- CPUのパフォーマンスは頭打ちになるので分散システムによるスケーリングを考える.\n- serverの数が増える = 障害点が増える\n- networkは制御できない\n- 機能横断要件の定義が分散システムでは必要\n  - availability\n  - response time and latency\n- Antifragile orgs\n- systems based failures\n  - timeout\n  - bulkhead\n  - circuit breaker (Hystrix etc)\n- Akka deal with DS failures\n\n- Akka\n  - 逐次処理ではなく非同期ベース\n  - messageを送信するだけ.その後は自分の仕事をするだけ.\n  - 1 actor 1 thread model\n  - 位置透過性 同一JVMであったりべつべつのJVMであったり\n  - hierarchy / let it crash\n\n- timeout\n  - tell (fire & forget)\n  - ask 応答が欲しい Future\n- circuit breaker\n  - 問い合わせ先のサーバが死んでいる場合は,retryのしきい値を超えると処理を切り離す\n  - state closed/open/half open\n  - decrease latency -> 無駄な問い合わせが減らせる\n  - CAP trade off\n    - read\n      - return old information\n      - don't return anything\n    - write\n      - just do my work\n      - need synchronize with others\n    - システム要件に適した設計を行う\n- bulkhead\n  - Akkaではblockingが必要になりそうなActorはActorごと分ける\n  - dispatcherのなかでblockingがあるとSleepやWaitingがふえるのでdefaultのdispatcherとは隔離する\n- Akka cluster\n  - node間でheart beatを投げ合って,nodeが死んでるか否かを確かめ合って死んでるnodeは切り離す\n  - akka persistenceでeventsを永続化して他のnodeでもevent情報を復元できる\n  - BackOff supervisor replay strategyを実装できる\n    - Capped exponential Backoff的なrandom性も設定できるので散らすこともできる\n  - split brain resolver\n    - ネットワークの影響でnodeが死んでるわけではないのにnodeが切り離されることがある\n    - このとき複数のclusterに分断されてしまうようなケースが発生する\n    - これを解決するのがsplit brain resolver\n      - 特定のnodeが含まれるclusterを残すとか4つの戦略がある\n      - 注意はこれはOSSではないのでLightbend Reactive Platformを利用する必要がある\n      - OSSでchatwork製のものもある\n- networkの影響による異常\n  - networkの影響でackが返ってこない時\n  - 分散システムでは複数回メッセージを送信しても問題ないようなべき等性を担保した設計が重要\n\n---\n\n# ストリーミング・ワールドのためのサバイバル・ガイド\n\n- Slide\n  - https://www.slideshare.net/ktoso/akkachans-survival-guide-for-the-streaming-world\n\n- memo取る余裕なかった.\n\n\n---\n\n# DMMのAPI GatewayをAkka StreamsとAkka HTTPで作り込んでみた\n\n- Slide\n  - https://speakerdeck.com/mananan/implementing-dmm-api-gateway-in-akka-streams-and-http\n\n- DMM EC site\n  - 2.5 billion PV/day\n- 認可パフォーマンスの向上がリプレースの主な目的\n- Architecture\n  - front Akka HTTP Server\n  - BackendのAPIをAkka HTTP Clientで叩いてfrontに返す\n  - StorageはすべてFuture対応\n- Akka HTTP\n  - low level APIを採用\n  - routingを自前で書けて今回の要件を満たすためにhigh level APIでは足りなかった\n- Routing flow\n  - Not Found Flow/Abort Flowを設ける\n  - Akka Streams's GraphDSL\n    - Partitionからn分割分岐させるjunctionを書ける\n  - Incoming/Outgoing Filter\n    - APIを叩く間に挟むFilter\n    - 社内事情をここで吸収する.(独自HeaderやREST/非REST APIの互換など)\n    - Akka HTTPのUnmashallerでHTTP Entityの解析を行う.便利だった.\n- Implementing Router with Graph API\n  - 最初の設計実装\n   - PartitionとMergeの間について\n     - Not Found Route * m\n     - Found Route * n\n     - Not Found/FoundがすべてMergeにたどり着く\n   - Partition sizeが増えるとHeapも増える\n   - GCもそれなりに起きていた\n  - 見直し後の設計実装\n    - Routeごとに１つのPartitionとMergeを割り当てる\n    - firstと比べて100-200M heapが減った\n    - GCの回数も減少\n  - FilterをまとめるのにflatMapConcatを利用\n- Testing the Performance Gateway API\n  - gatlingを利用\n  - GatewayのBackendにFast Endpoint/Slow Endpointの２つをエミュレートしたサーバを配置した負荷テストも実施\n    - Gateway全体のスループットが落ちないかを検証するため\n- Summary\n  - back-pressure controlを利用するとgateway実装としてAkkaは有効\n  - Akka HTTPによるProxyはGatewayの特性に適合している\n- Q&A\n  - Graph構造の中で例えばBlockingが入るような重たい処理が入るときはどう制御している?\n    - Blockingが入るような重たい処理はdispatcherを分けている\n  - Graphの中で制御できない予期できない例外が発生した場合はどうなる?\n    - Akka HTTPで503等のResponseを返すことが保証できる\n\n---\n\n# Akka Streams による Kafka の Reactive 化\n\n[@kpciesielski](https://twitter.com/kpciesielski)\n\n- akka-stream-kafkaのコミッタ\n- kafka\n  - producerがtopicを書き込み,consumerが消費する\n  - topicはpartitionで分割できる\n  - partitionはコピーではない\n  - consumerに複数のpartitionからtopicを送信することをbalancingと呼ぶ\n    - consumerはpartitionが複数あることは意識しない\n  - consumerはclusterを組める\n    - groupのような識別子でグルーピング\n    - kafkaは同じclusterに所属するconsumerがある場合は並列にそれぞれに送信できる\n  - consumerとproducerは粗結合\n    - この特徴がmicroservicesで重要\n  - consumerはあるタイミングでACKを返すことができる\n    - kafkaはACKをうけとるとコミットする?\n- Akka Streams\n  - アクターモデルを使用したデータ変換パイプラインDSL\n  - backpressure\n  - powerful testkits for async\n  - 拡張可能(様々な技術のコネクターを書ける)\n  - Stage\n    - Input: Source\n    - In/Out: Flow\n    - Output Sink\n- Akka Streams + Kafka\n  - Consumer: Source\n  - Producer: Sink\n  - Producer: Flow writeに対してもbackpressure\n  - Akka ecosystem内で開発中\n  - examples\n    - plain consumer: commitなしのSource?\n      - play Java Kafkaを利用するとReactive性が落ちる.ループを書いてポーリングするような処理が必要.\n      - Akka plain consumerを使った実装のほうがplain consumerの実装とほぼ同じパフォーマンスを出せる.\n    - commitable source\n      - writeではBlockingが必要\n      - 100件とかになるとbufferingして一度にcommitとかを検討する(Batch)\n      - Akka Streams Kafkaでは`mapAsync`, `grouping`等を利用してAt-least-onceを保証する.\n      - Akka batched consumerのメッセージ処理パフォーマンスはまだbatched consumerに届かない.\n    - producer\n      - plain JavaのAPIを利用すると処理をCallbackを用いて書く必要が出てくる.Callbackは管理が難しくなる.\n      - Akka Streams KafkaではMessageを作成して次のStageに流していくような記述ができる.\n  - partitionごとのbackpressure: `Consumer.commitablePartitionSource(consumerSettings, Subscriptions.topic(\"topic1\"))`\n    - `map`メソッドでパターンマッチで`partition`情報などが取れる\n  - Error handling\n    - `toMat`でマテリアライズする\n    - `streamFuture.onFailure`で例外を処理できる\n    - `control.shutDown()`でシャットダウンもできる\n  - Kafka Streams\n    - Apache Kafka製の別物のライブラリがある\n    - Java, stateful processors, windowing, joining, aggregation\n    - Akka Streams Kafkaはasyncにフォーカスしている\n\n---\n\n# Scala ＆ Sparkによるデータ・エンジニアリング 7大レシピ\n\n[@ahoy_jon](https://twitter.com/ahoy_jon)\n\n- 1. organization\n  - データエンジニアリングではデータを移動したり取得したりするツールに問題があると思われがち.\n  - しかし、組織が問題になることにも目を向ける必要がある.\n  - ビジョンを共有しないそれぞれのチームにデータを公開すると,その依存を考えながら仕事をすることになる.\n    - ビジョンを明文化することが重要.\n    - チームとしてチームを超えて協力することを明確にする.\n- 2. Work optimization \n  - リードタイム: 企画から完成までの期間\n  - インパクト: 今の文脈を超えた良い効果\n  - 失敗の管理: \n  - 何かを処理している間(MapReduce)に待つ(Wait)ことを減らすには?\n    - あるコンポーネントで何が失敗しそうか考え,その頻度や予防策,復旧プランを考える\n- 3. Staging Data\n  - ステージングは永続化構造として見えるようにすべき\n \n- 5. Cogroup\n  - IOモナドに似ている\n  - data連結にとても有用\n  ```scala\n  from(left: RDD[(K, A)], right: RDD[(K, B)])\n    .join()\n    .outerJoin(Option[A], Option[B])\n    .cogroup(Seq[A], Seq[B])\n  ```\n\n- 6. Inline data quality\n  - データ品質を高めるとバッドデータへのレジリエンスを高める?\n  - Annotationsはaggr\n- 7. Real programs\n  - データパイプラインの各パートはstatelessにデザインすべき\n\n---\n\n# ChatWorkのScala採用プロダクト “Falcon” リリースまでの失敗と成功歴史\n\n- ユーザが生成するデータのスケール遷移\n  - 1年前10億メッセージ、直近だと18億メッセージ\n  - サービスの成長速度を優先し技術的負債は他社事例にあるように例外なく貯まる\n  - サービス成長とともにメッセージを捌ききれなくなりサービスダウン等を引き起こす\n- Phase1: Live migration\n  - 既存の安定しているシステムに影響を与えない\n  - 新システムではダウンタイムなしにmigrateしたい\n  - 既存データのmigrationはしない\n  - プロジェクトの終盤で様々な問題が発生して一旦プロジェクトを停止\n  - 3日使って振り返りを行う\n- Rebooting the project\n  - projectの体制を見直しマネジメント面を強化\n  - Chatworkのインフラとなるのでrobustのものを目指す\n  - PoC\n  - システムが満たすべき要件\n    - Scalabiity\n    - Resiliency\n    - concurrent connectionsとthroughput2倍を目指す\n    - Low cost\n    - DDDをベースとした機能性\n- PoC\n  - chat roomとmemberを含むメッセージ機能にスコープを決める\n  - 圧倒的にReadが多いのでCQRS\n  - Writeはcasandra.Akka cluster/Akka HTTP/Akka Stream/Akka Persistence\n  - ReadはAWS Aurora. Akka HTTP/Akka Stream/Skinny ORM\n  - ReadとWriteの同期は非同期にイベント駆動で\n  - パフォーマンス的にはなかなかの結果が得られた\n  - Akka clusterは運用にのせるためには回避出来な問題がわかり見送りに.(split brain)\n  - Casandraでは障害が発生したnodeを復旧するために24時間という見積もりに\n  - Auroraはsingle masterでは書き込みがスケールしない.shardingで改善もできるが開発の難易度が高く属人化しやすい.\n- Phase2: Re-Architecture from PoC\n  - akka clusterは運用コストから見送りに\n  - Write DBとしてCasandraではなくKafkaに置き換える\n  - Read DBはAuroraからHBaseに変更\n  - chatworkではmessaingがコアドメインなのでここにフォーカス\n  - DDD Context MapがPhase1よりシンプルになったためコミュニケーションコストが下げれた\n  - AuroraからHBaseへのdata migration\n    - Basic migration: 最終メンテから4日前までのすべてのデータを以降\n    - Diff migration: Basic migrationの対象期間以降のデータ\n- DevOps\n  - kubernetes\n  - kube-aws AWS上にコンテナクラスター組める\n  - Jenkins -> Concourse CI (developed by pivotal)\n- Finally Release\n  - 低遅延等、高スループット等はいい結果が得られた.\n  - 今後も改善\n  - Scala community応援ありがとう!\n\n---\n## あとがき\n\n- セプテーニ・オリジナルの技術読本の「Akka HTTPでLINE bot作ってみました」はサンプルとして面白かった.\n- スタッフのみなさんお疲れ様でした!\n"
                                      }
                                    }
                                  ]
                                }
                              }
                            ]
                          }
                        },
                        {
                          "type": "tree",
                          "name": "03",
                          "object": {
                            "entries": [
                              {
                                "type": "tree",
                                "name": "16",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "reactjs-reactnative-meetup.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/d87a14f234ab3728fec459478d021143ec39a55a",
                                        "text": "React.js meetup × React Native meetup\n===\n\n- https://react-native-meetup.connpass.com/event/49024/\n\n---\n\n# パネルディスカッション\n\n- @yosuke\\_furukawa\n- @koba04\n- @yoshitosi\n- @janus\\_wel\n\n## What's React Native\n\n- Native experience\n- Learn once, write anywhere\n- Keeping development velocity\n\nWeb上でコンポネントを動かすことも可能だが、Nativeで提供できる体験にはかなわない。\n基礎的な技術を学び直すことなくプラットフォーム固有の個々のアプリを開発する。\n\n## React Nativeに注目したきっかけ\n\n- Recruit\n  - 1・2年前から注目はしてた。\n  - Reactを全面的に会社で採用。\n  - React開発者のバックアップが得られそう。\n  - Nativeの開発者もそもそも足りていない。\n  - React Nativeに注目している開発者が身近にいた。\n- Togetter\n  - TogetterではiPhone/AndroidのNativeエンジニアは社内にいない。\n  - Webの開発者はいたが、React NativeがAndroid対応したあたりからやり始めた。最初はReact Native Androidの方はバグもあった。\n  - 現在は2人でiPhone/Androidアプリをリリース。\n- CureApp\n  - CureAppでは初代CTOがJS好き。アプリはtitaniumだったけど辛くなってきた。\n  - 別のスタンドアローンのプロトタイプをReact Nativeで実装して見せたら感触が良かった。\n  - そこから2・3人で開発を初めて今にいたる。\n\n## Learn once, write anywhereは実際どうなの?\n\n- Reactから入るひとにはしきいは高くないという解釈。\n- Nativeへの繋ぎこみ部分だけ薄く提供されているのでNative機能を検索すればつくれてしまう。\n\n## React Nativeでどこまで共通化できる?\n\n- Platformをまたぐ共通化コンポーネントはものすごく低レベルのボタンぐらいしかWrite anywhereは期待していない。\n- Write anywhereよりもReact開発者がなんとなく開発できるというところには期待している。\n- React Native WebはViewを抽象化してWebでもNativeでも使えるというものがある。\n- 画面レベルでのコンポーネントは分ける必要がある。Platformごとにエントリーポイントが異なる?\n- TogetterではPlatformごとのコンポーネントによせるということはまだできていない。アプリごとにコードはほぼ同じ。\n- WebとNativeでの共通化はどう考えている?\n  - PhoneGapで昔は考えたことあるがもうやめた。\n  - React Nativeの採用をCureAppでは考えている。というのもアプリの中にDSLを使ってそれをパースしてごにょるみたいな部分がある。この部分のデバッグがNativeだときついのでReact Native Webならどうかと。実際にやっているひとの話を聴いた感じだとイケるかもしれないと考えている。\n- パフォーマンスは?\n  - CureAppでアプリをためしにつくったときは60fpsぐらい出た。\n  - WebのPhoneGapやTitaniumよりは全然パフォーマンスがよい。\n\n## バージョンアップはどう?\n\n- 今のWebのReactの問題は複雑なViewの差分計算が同期的に行われていてパフォーマンスがでない点。\n- 16からはReact fiberでこの問題を非同期処理で行うようにする。フラグで管理。\n\n## Reactってあと何年使える?\n\n- Reactが死ぬのと一緒にReact Nativeも死ぬと思う? > React Native使っている勢はどう思っている?\n- Togetterでは何年使えるというよりは速くリリースできることが良かった。\n  - アプリ作って見るとページへの滞在時間が3倍になったとかが素早くフィードバックを得られた。\n  - これでアプリをつくれば価値が出せそうだという所感を得たのでNativeアプリ開発がんばってみるかというフェーズ。\n- furukawa氏もReactとReact Nativeの寿命は一緒かなと考えている。\n  - 何年使えるっていうよりも、Web開発者がNativeの知るきっかけになると思っていてそこにやる価値はあると思っている。\n- React NativeのcontributorsはFacebook外に多い。\n- CureAppのアプリでは、患者さんを賢くサポートするという機能が求められる。\n  - これを各Platformの言語で書くとリソースが追いつかなくなってしまう。\n  - これをJavaScriptとDDDで書くことでスタートアップが取れる戦略としてはありかと思っている。\n- Backendも\n\n## 結局、React Nativeやりますか？\n\n- DeNAのng-coreはReactNativeと野望は一緒だった。\n  - ng-coreはとんざしてしまったけど、React Nativeはng-coreが超えられなかった課題を超えられるかには着目している。\n  - Closed sourceだった React Native OSSなのでこれは○\n  - Platformのアプリ審査が必要だけどこれはベンダー側がOKしないので課題としては残ったまま。\n  - Unityのようなオーサリングツールとの相性や繋ぎこみがないと本格的なゲームはつらい。\n- 今のNative engineerにReact Nativeの普及や導入を手伝ってもらう流れにならないと大きな波にはなりづらいかも。\n- Windowsのコードプッシュ?はどう?\n  - バグ回収とか早いリリースにはよさそうだけど、悪いコードに差し替えるとかいう事例が出てくるとベンダーにバンくらう可能性はある。\n- ReactエンジニアがWebの強みからNativeへスキルを伸ばしていくというのは価値があると思う。\n\n\n# React Hot Loaderを使って開発をさらに加速させる\n\n- stateを保持したままコード更新を反映させることができる。\n\n# Inside Bdash\n\n- BdashではDomainLogicはView以外のデータとってくるとかの処理全般と定義。\n- BdashではAction creatorsからDomain Logicを呼んでる。\n- BdashではStoreの分割はページ(Container)単位。\n  - この設計だとページをまたいでデータを共有するのが面倒。\n  - ページ間で共有するデータはDomain LogicにStore(LocalStorageとかをバックエンドに)として保持。\n  - nestedなstateの更新ユーティリティとしてimmupを作った。\n\n# HyperApp - 1KBのビューライブラリ\n\n- react, preact, inferno, mithril, ember, vueに似ている。\n- hyperapp = vdom + redux/elm的なstate管理 + router\n- 〜1kb\n- Reactはheavyすぎ\n- vdomエンジンが欲しかった\n  - OSSのやつはhyperappに適した奴がなかったので自作\n- dependenciesは0\n- 300行のコードベース\n- next?\n  - vdomエンジンのパフォーマンスのためにrequestAnimationFrameを使う。\n\n\n# 小回りの効くWebViewの使い方\n\n\n# React nativeでfirebaseのネイティブSDKを操作する \n\n\n# Androiderから見るReact Native\n\n- dev env\n  - WebStorm\n- React nativeをやっているとアプリに必要なコンポーネントを洗い出してComponentとして提供されているか、Native API bridgeを探す作業のループ。\n- 良いと思ったところ\n  - ごりごりにパフォーマンスだしたいところはNativeで、面倒な作りあきた部分をReact Nativeで作ってくれると同居できる。\n- Native固有の動きをReact Nativeで実装できるの?\n\n"
                                      }
                                    }
                                  ]
                                }
                              },
                              {
                                "type": "tree",
                                "name": "30",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "shibuya-xss-9.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/6f53852ff11a74bcf94e2477d8cb4a9de9b3f094",
                                        "text": "Shibuya.XSS techtalk #9\n===\n\n# Edgeの話\n\n- Edgeはソーシャルネットワーク型のマルウェアの取れる検出率が高かったのでChromeより安全と謳っている。\n- なので探したｗ\n- Windows 10 insider preview, Fast ring, slow ring\n- Let's get started!\n  - SOP\n    - Edgeはportが変わってもSOとみなす\n    - Edge側としては後方互換性のためで将来的にはスタンダードに合わせるかも\n    - iframeからのredirectを誤認させる挙動\n  - Edgeの独自機能を見ていく\n    - Linkを踏ませてWeb noteを取らせると(local file)Web noteからデータを引っこ抜ける\n    - location.href内にPCユーザ名があるのでそれを狙う\n    - Web noteにはスクショとかData URLで引っこ抜ける\n- 今年のFile関連脆弱性で注目しているのはEPUB\n  - EPUBはzipアーカイブ\n  - EPUBはWeb技術を使用している HTML,CSS(,JavaScript)\n  - AppleのEPUBリーダはJSをサポートしている\n  - 画像を使えば外部にリクエストを遅れてlocation.hrefも取れるのでユーザ名等が引っこ抜けた。\n  - AdobeのEPUBリーダは後ろはIEっぽい\n  - IEはPortが異なってもSOとみなすのでlocalhostの全サービスにアクセス可能\n  - EdgeのEPUBリーダはiframe sandboxを使ってユーザコンテンツを表示しておりJSの実行を制限している\n  - iframe sandboxをバイパスできればJSをEPUBで実行できる\n  - EPUBリーダーによってはハッキングツールと化す恐れあり\n  - ユーザはリーダーの実装を理解しないとデータを守れない\n  - EPUBの仕様に従っているリーダはない(らしい)www\n  - XXEの脆弱性も上がっていてサーバのデータも...\n\n# WAFを自作した話\n\n- Vurp\n  - Webアプリをリバプロして脆弱性も持つWebアプリにできるものｗ\n  - 何に使う?\n    - セミナーのデモ\n    - トレーニングでの演習環境\n\n# XSSフィルターの使い方\n\n- https://speakerdeck.com/masatokinugawa/shibuya-dot-xss-techtalk-number-9\n\n- XSSフィルターの基本\n  - ページを書き換えることによりXSSを防ぐ\n  - XSSフィルター,Auditor,FFではoScriptNを使うと同等\n  - IEやEdgeは#で置換するという動作\n  - ChromeやSafariは一部を消す\n  - NoScriptは事前に危険なリクエストを置換\n  - 誤検知は仕組み上さけられない\n  - この誤検知を利用して攻撃できないか?\n  - フィルタの誤動作が起こることを利用する\n- X-XSS-Protectionヘッダは1以外を利用しよう\n\n# WebセキュリティとW3CとIETFの仕様\n\n- https://www.slideshare.net/yuki-f/web-w3cietf\n\n- W3C\n  - Web Application Security WG\n- CSP Level3\n  - strict-dynamic\n  - disown-opener\n  - navigation-to\n  - report-sample\n- CSP Embedded Enforcement\n  - 埋め込む側がiframeの中のコンテンツにCSPをつけるように要請する\n- Clear Site Data\n  - サーバからクライアントのCookieやキャッシュをクリアできる\n- Suborigins\n  - 同一オリジンで複数アプリケーションが提供される場合にサブオリジンを指定できる\n  - Access-Control-Allow-Suborigin\n- HSTS mi\n- CORS and RFC1918\n  - パブリックなネットワークから、ローカルネットワークへのCSRFをできないようにする\n- Isolated Origins\n  - HTTPヘッダでIsolation=1を受け取るとUAが特定の動作をするようになる\n- Origin Policy\n  - オリジン全体にポリシーを適応できるようにする\n  - ヘッダにポリシー名を指定しファイルを置く\n  - Sec-Origin-Policy: \"policy-1\"\n  - CSPはレスポンスごとに設定する必要があるがだるいなど\n- Cookie Prefixes\n  - Cookieについてる属性を確かなものにする\n  - Set-Cookie:__Scure-SID=blahblah\n- Same-Site Cookie\n  - laxとか\n  - CSRF対策になる\n- Let localhost be localhost\n  - localhostをループバックアドレスにする仕様\n    - secure contextで\"localhost\"をsecure contextにしたいというモチベーション\n    - localhostがループバックアドレスを返すことをMUSTにしたい\n- ブラウザのセキュリティ系Mailing Listはおすすめ\n- Google のMike Westのレポジトリとアクティビティも良い\n"
                                      }
                                    }
                                  ]
                                }
                              }
                            ]
                          }
                        },
                        {
                          "type": "tree",
                          "name": "06",
                          "object": {
                            "entries": [
                              {
                                "type": "tree",
                                "name": "07",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "workflow-engines-night.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/8a5daa4f4dbe1baa3f3e9d5e94d60ef9e13541eb",
                                        "text": "Workflow Engines Night\n===\n\n- [ATND/Workflow Engines Night](https://atnd.org/events/88470)\n- [hashtag #workflowenginesnight](https://twitter.com/search?f=tweets&vertical=default&q=%23workflowenginesnight&src=typd)\n\n# StackStorm ワークフローエンジンによる運用コスト・リスク低減の取り組み by 大山 裕泰 氏\n\n## システム運用者を悩ませる課題\n\n- タスク要求処理\n- 報告\n- 部内調整\n  - 社内DNS運用者などがアクター\n  - いろんなチャネルから依頼が来る\n- 個別・具体的なシステムへのオペレーション\n  - システムが変わった時に教育コスト等あらたにかかる\n- 定形運用のAPI化\n  - フリーフォーマットからカタログ化したオペレーションのAPIを提供\n- システムの抽象化\n  - 例えば、依頼者はVMが欲しいわけではなく、IP reachableなマシンが欲しいはず\n\n- StackStorm\n  - Netflix製\n  - IFTTT × WorkFlow\n    - e.g. If GitHub Trigger then Jenkins action something.\n  - An external world -> Trigger -> Sensor -> Rule -> Workflow -> (Multi) Action -> Another external world\n  - Packというmoduleが提供されている(Rubyでいうgemのようなイメージ)\n    - e.g. GitHub\n    - Provided nearly 10,000,000 packs\n- Q&A\n  - exactly onceはどう実現する?\n  - on failureの柔軟性は?\n\n# Digdagへ日次バッチを移行して幸せになる話 by @i_szyn\n\n- [speakerdeck/JenkinsからDigdagへ日次バッチを移行して幸せになるお話](https://speakerdeck.com/dmmlabo/jenkinskaradigdagheri-ci-batutiwoyi-xing-sitexing-seninaruohua)\n\n## なぜDigdag?\n\n- 他事業部のデータ取り込み処理\n- データ加工処理\n- BIレポートむけデータ集計処理\n- Extract(Python) -> Transform(Hive query) -> Load\n- 脱Jenkins日次バッチ\n- 要件\n  - 可用性 -> Posgresqlを落とさなければ○\n  - スケーラビリティ-> 並列処理\n  - 処理フローがコードで管理できる\n  - リトライ可能 - 失敗処理\n  - 進捗確認ができる -> Rest API\n\n## システム構成\n\n- Postgresql(Primary standby構成 Streaming replication)\n- Pgpool-Ⅱ(Active standby構成 Watchdog)\n\n- Digdag server(兼client)を2台でlsyncd & rync\n\n## Workflow設計\n\n- 事業単位で１つのワークフローを構成\n  - e.g. ゲーム事業\n\n- Web UIで実行時間とかわかる\n\n## Pluginの話\n\n- slackオペレータ\n- myui/digdag-plugin-exampleが参考になる\n\n## チーム間連携\n\n- Polling -> Success -> (another task) Start\n- mogという自社CLIでタスクのstatusを取得可能/startもできる\n  - mogによりタスクが処理時間依存だったのを解決できた\n\n## ハマリドコロ\n\n- 処理時間が長いWorkflowだと、DigdagのDatabase接続のSocket Timeout\n\n## Q&A\n\n- dev/prodの切り替えは?\n  - digdagの起動時パラメータで解決\n- task実行中にdigdag serverが死んだときどんな動き\n  - taskのStateをDBに保存して、それをPollingでやりとりしている\n  - ShellやRuby Scriptは例外\n- Taskを実行しないdigdagとTaskを実行するdigdagという構成が古橋さんのおすすめ\n  - `--disable-local-agent` optionを利用する\n\n# Digdagによる大規模データ処理の自動化とエラー処理 by @frsyuki\n\n## Slide\n\n- [Digdagによる大規模データ処理の自動化とエラー処理](https://www.slideshare.net/frsyuki/digdag-76749443)\n\n## What's workload automation?\n\n- あらゆる手作業の自動化\n\n## Challenge: Modern complex data analysis\n\n- Ingest -> Enrich -> Model -> Load -> Utilize\n- このdata analysisのフローで複数クラウドが当たり前なのだが、これをクラウドに依存させない形で解決したいが大元のモチベーション\n\n## Run anythingのために\n\n- myui/dockernized-digdag-server\n\n## 有向グラフではloopが書けない\n\n- digdagは動的にタスクを増やせるのでloopが書ける\n\n## Digdag at TD\n\n- 850 active workflows\n- 3600 workflows run every day\n\n## Q&A\n\n- server mode: clientからpushされたTaskを実行される\n- scheduler mode: localにあるWorkflowをリロードして実行する\n  - 手元で動かしたいときに便利\n- REST APIの仕様は今後かわらない。なぜならTD社でもう動いているから。\n\n# digdag導入でよかったこと&ハマったこと by 塩崎 健弘 氏\n\n- [speakerdeck/Digdagを仕事で使ってみて良かったこと、ハマったこと](https://speakerdeck.com/shiozaki/using-digdag-in-production-environment)\n\n- UbuntuだとPythonオペレータでpython2が実行される\n- embulk gem manager的なやつが欲しい\n- src読んで発見する機能がある\n\n# とあるマーケティング部隊でのdigdagの活用事例 by grimrose\n\n- [gist/とあるマーケティング部隊でのdigdagの活用事例](https://gist.github.com/grimrose/5bea98db4c82056dad1ab84d6653308e)\n\n- operatorでエラーになっても原因がわかりにくい\n"
                                      }
                                    }
                                  ]
                                }
                              },
                              {
                                "type": "tree",
                                "name": "24",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "manage_data_in_docker_containers.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/6ac381c2df28bdd61485351ce1e66600fe4bb00b",
                                        "text": "Docker containerにおけるデータの取り扱い\n===\n\n#### materials\n\n- [Manage data in containers](https://docs.docker.com/engine/tutorials/dockervolumes/)\n\nDocker engineを利用したcontainer間、container内のデータを取り扱う方法は主に２つある。\n\n- Data volumes\n- Data volumes containers\n\n## Data volumes\n\nData volumesは**Union File System**をバイパスする一つ、または複数のcontainerにおける特別にデザインされたディレクトリ。データの永続化や共有のためにいくつかの特徴を提供する。\n\n- containerが作成されたときにVolumesは初期化される。特定のマウントポイントにおいてcontainerのベースイメージがデータを所持していれば、Volumeの初期化において新しいVolumeに既存データはコピーされる。ただし、ホストディレクトリにマウントするときはこれは適用されない。\n- Data volumesはcontainer間で共有・再利用ができる。\n- Data volumesへの変更は直接行われる。\n- Data volumesへの変更はイメージへの更新をする際には含まれない。\n- container自体が削除されてもData volumesは永続化されている。\n\n## Add a data volume\n\n`docker create`や`docker run`コマンドにオプション`-v`をつけて複数のdata volumesを利用することができる。\n\n```bash\n$ docker run -d -P --name web -v /webapp training/webapp python app.py\n```\n\nDockerfileで`VOLUME` instructionを利用して1つ以上の新しいvolumesをcontainer作成時に指定することができる。\n\n## Mount a host directory as a data volume\n\nDocker engineのホストのディレクトリをcontainerのディレクトリにマウントすることもできる。\n\n```bash\n$ docker run -d -P --name web -v /src/webapp:/webapp training/webapp python app.py\n```\n\n上のコマンドでは、ホストの`/src/webapp`ディレクトリをcontainerの`/webapp`ディレクトリにマウントしている。\nすでにcontainer内に`/webapp`ディレクトリがあれば、そのまま利用して既存のデータを削除しない。\n\ncontainerのディレクトリ指定は常に`/path/to/dir`のような絶対パスでなければならない。\n\nホストのディレクトリは絶対パスや一意に決まるnameを指定する。nameを指定するとDockerが作成する。nameはアルファヌメリックで始める必要がある。\n\n次のようにDocker containerのディレクトリに対しread-onlyを指定することも可能。\n\n```bash\n$ docker run -d -P --name web -v /src/webapp:/webapp:ro training/webapp python app.py\n```\n\nまた、`cached`を指定すると読み込みが重たくなるような処理でパフォーマンス改善につながりうる。ただし、ホストとの一時的な一貫性が失われる。\n\n```bash\n$ docker run -d -P --name web -v /src/webapp:/webapp:cached training/webapp python app.py\n```\n\n## Volume labels\n\nSELinuxのようなlabeling systemでは、containerに適切なlabelを配置する必要がある。\n\n## Mount a host file as a data volume\n\n`-v`オプションではディレクトリの他にファイルを指定することもできる。\nユースケースとしてはホストの`.bash_history`をcontainer内で利用するなどが挙げられる。\n\n## Creating and mounting a data volume container\n\ncontainer間でデータ共有をしたり、non-persistent containerからデータを利用したい場合、**named Data Volume Containerを作成して、そこからデータをマウントする**のが最もよい方法。\n\ndockerのstorage driver等は以下を前提とする。\n\n```bash\n$ sudo docker info\n\nServer Version: 17.03.1-ce\nStorage Driver: aufs\n\nPlugins:\n Volume: local\n\n```\n\nまずはVolumeを作成する。\n\n```bash\n$ sudo docker volume create --opt o=size=50G elasticsearch-data\n```\n\nvolumeをリストする。\n\n```bash\n$ sudo docker volume ls\n```\n\n生成されたvolumeの構成を見る。\n\n```bash\n$ sudo docker volume inspect elasticsearch-data\n[\n{\n    \"Driver\": \"local\",\n        \"Labels\": {},\n        \"Mountpoint\": \"/var/lib/docker/volumes/elasticsearch-data/_data\",\n        \"Name\": \"elasticsearch-data\",\n        \"Options\": {\n            \"o\": \"size=50G\"\n        },\n        \"Scope\": \"local\"\n}\n]\n```\n\n\n生成したvolumeは以下のコマンドで削除できる。\n\n```bash\n$ sudo docker volume rm elasticsearch-data\n```\n\n\n"
                                      }
                                    },
                                    {
                                      "type": "blob",
                                      "name": "storage_terminologies_on_linux.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/b69b20f2ef8386b719fbaa0769da9a08ed40626d",
                                        "text": "LinuxにおけるStorageにまつわる用語整理\n===\n\n# Block devices\n\nBlock deviceはある大きさのブロック単位で、ランダムにデータの読み書きを行えるデバイス。ハードディスクやUSBメモリやCD-ROMなどのディスク装置全般がBlock device。\n\nLinuxでは以下のコマンドでブロックデバイスをリストできる。\n\n```bash\n$ lsblk\n```\n\n# filesystem\n\nfilesystemはBlock deviceにおけるデータ操作(読み書き)を行うためのOSの機能。\nUnix系OSでは仮想的なファイルシステムを生成する。\nUnix系OSはルートディレクトリをひとつ持ち、すべてのファイルはルートディレクトリ以下に配置される。\n[\"On a UNIX system, everything is a file; if something is not a file, it is a process.\"](http://www.tldp.org/LDP/intro-linux/html/sect_03_01.html)\nブロックデバイス上のファイルにアクセスするために、それらをディレクトリツリー上のどこに置くかを指示する必要がある。これをマウントと呼ぶ。\nマウントポイントの決め方には慣習がある。[File Hierarcy Standard](https://ja.wikipedia.org/wiki/Filesystem_Hierarchy_Standard)。\nLinuxでは、現在のkernelでサポートしているfilesystemは\n\n```bash\n$ cat /proc/filesystems\n```\n\nで確認できる。\n\n# Partitions\n\nPartitionはハードディスクを分割したlogical diskのこと。`fdisk`コマンドを利用する。\n\n# Volumes\n\nVolumeはハードウェア上の記憶領域。\n\n# LVM\n\nLVM(Logical Volume Management)はlogical volumeやfilesystemを管理するシステム。diskをひとつ以上のセグメントにパーティショニングしたり、それらのPartitionをfilesystemを利用してフォーマットするtraditionalな手法よりも高度で柔軟。\nキーコンセプトは３つ。Volume Groups、Physical Volumes、Logical Volumes。\n\nVolume GroupsはPhysical VolumesやLogical Volumesの名前のついた集合。\n\nPhysical Volumesはdiskに相当する。Logical Volumesを保持する領域を提供するBlock devices。\n\nLogical Volumesはpartitionに相当する。Logical Volumesはfilesystemを持つ。partitionと異なり、番号よりも名前を取得し、複数のdiskにまたがっても良いし、物理的に連続である必要はない。\n\n\n#### Refs\n\n- [Block devices](http://itpro.nikkeibp.co.jp/article/Keyword/20081023/317625/)\n- [ファイルシステム @ Wikipedia](https://ja.wikipedia.org/wiki/%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0)\n- [LinuxFilesystemsExplained @ help.ubuntu.com](https://help.ubuntu.com/community/LinuxFilesystemsExplained)\n- [Lvm @ wiki.ubuntu.com](https://wiki.ubuntu.com/Lvm)\n- [man5 @ manpage.ubuntu.com](http://manpages.ubuntu.com/manpages/xenial/man5/fs.5.html)\n- [lvm manpage @ die.com](https://linux.die.net/man/8/lvm)\n- [fdisk manpage @ die.com](https://linux.die.net/man/8/fdisk)\n\n"
                                      }
                                    }
                                  ]
                                }
                              }
                            ]
                          }
                        },
                        {
                          "type": "tree",
                          "name": "07",
                          "object": {
                            "entries": [
                              {
                                "type": "tree",
                                "name": "26",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "aws-startup-seminar.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/da2891c94b504ea19aa68796441e8e245e06b37b",
                                        "text": "# ユーザ動向を分析したい\n\n- とりあえず生ログはS3に貯めておく\n- まずはGAで足りることを\n- 要件が満たせなくなったらログに手を出す\n  - やりたいことにはどれが向いているか\n  - Athenaは単純な縦方向のクエリに\n  - RedShiftはGroup byしまくるような複雑なクエリに\n\n# CI/CDをば\n\n- コンテナ使いたい\n  - 12-factor App\n\n# 運用監視ちゃんと\n\n- AWSの各サービスにはモニタリングツールがあるのでそれをまず把握する\n  - Healthy Hosts\n  - Replica slow log\n- アラームを設定\n  - ELBのレイテンシが500msec超えるとUXに影響があるので監視するなど\n- 通知をしつつオートスケーリングさせる\n  - Lamdaを使って通知が来た時に何をするかを定義してやらせる\n- DevSecOps on AWS movie良いよ\n\n# システム負荷下げたい\n\n# Growth Hack (mobile)\n\n- Aamazon Pinpoint強い\n  - カスタムイベント、カスタム属性をアプリから送信できる\n\n# コスト下げたい\n\n- リザーブドインスタンスとスポットインスタンスの使い分けは？\n  - 最低稼働数を参考に\n\n---\n\n# トレンド\n\n- Amazon Lightsail\n  - 月額制\n  - すぐBrowserからSSHできて、スモールスタートに向く\n- AWS IoT\n  - 100万メッセージで月800円程度!\n- Mobile\n  - 認証サービスなど\n- 動画\n  - Adaptive Bit Rateでえしちょうしゃの回線状況、解像度によって自動的に適切な品質で配信\n- サーバレス\n  - イベントドリブン\n  - 無意味なポーリングや定期起動の必要性がなくなる\n- FinTech\n  - FISC対応/PCI-DSS セキュリティ監査\n  - どこまでやるか?\n    - AWSのセキュリティベストプラクティスを守っていればIPOも問題ない\n"
                                      }
                                    }
                                  ]
                                }
                              },
                              {
                                "type": "tree",
                                "name": "28",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "idcon-vol23.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/74890ec27b8969b9440328cba63dd9b2c2c54cd1",
                                        "text": "#idcon vol.23\n===\n\n# Credential Management API by @agektmr\n\n- パスワードがもれないようにするには?\n  - one-time\n  - complecity\n  - 使いまわさない\n  - ユーザの啓蒙\n  - password manager\n  - onepassword\n  - passwordを覚えるのは人間には苦痛\n  - passwordを使わないのが理想\n  - identity/social login\n- goals\n  - 強いクレデンシャル\n  - password manager\n  - 優れたUX\n- Smart Lock for Password\n  - Android native app向け\n  - Chromeのpassword managerを利用して、Nativeでログインできる e.g. Netflix\n- Credential Management API\n  - Chromeで先行実装\n  - Web標準化に向けWIP\n  - Firefox, Safariで実装予定, Webkitも\n  - JavaScriptでクレデンシャル情報を操作\n  - ワンタップログイン\n    - 複数アカウントがある場合は選択、１つのときは即ログイン\n    - ログアウト時に自動ログインの制御もできる\n    - AliExpress\n      - Credential Management APIを使うことでログイン率41%Up\n      - ログイン失敗率85%Down\n  - Usage\n    - クレデンシャルを保存する\n    - social loginの場合はtokenではなく、どのアカウントでログインしたかを保存する\n  - サブドメインをまたいだクレデンシャルの共有\n    - Same originではないと基本脆弱性\n    - SSOしたい場合があるのでサブドメインが同じであればOKになった\n  - Top level domain(TLD)をまたいだクレデンシャルの共有\n    - Digital Asset Links\n      - ドメイン間の関係を定義してあげることでドメイン間でのクレデンシャルの共有ができるようになる\n    - ニーズは国ごとに違うドメイン(google.com, google.co.jp)を持っている会社からなので想定ユースケースはこれ\n      - このユースケース以外においてパスワードの使い回すための仕組みではない\n- CM APIではgoalsの強いクレデンシャルを満たせない\n- そこでSign Up API\n  - ユーザが過去にログインに使用したアカウントを選択させる\n  - Google accountを選択した場合は、GoogleとOIDCでfederationしてtokenを取得する\n  - 現在Identity Teamでプロプライエタリ環境でJS実装中\n  - 試したい方は専用Formに登録すると先行実験利用できるかも\n- Openさ\n  - 紹介したAPIはGoogleのPassword managerに依存している\n- `navigation`のnamespace以下に、Credential Management APIの他にWeb Authentication APIも実装される?\n  - Web Authentication APIはMSでやっているFIDOをJSのInterfaceを定義してW3Cに持って行っているもの\n  - https://blogs.msdn.microsoft.com/tsmatsuz/2016/06/08/w3c-web-authentication-api-javascript/\n\n# Payment Request API by @agektmr\n\n- Why Web Payments?\n  - よりよいUX\n  - 手軽に実装できる\n  - 新しいecosystem\n- 問題提起\n  - フォームにクレカ番号を入力してサブミット\n  - mobileでフォームに入力するのは苦痛 -> 滅びて欲しい\n- Payment Request API\n  - 支払いのためのWeb兵十ン\n  - 支払い専用にブラウザネイティブの一貫したUIを提供\n  - それ自体は支払い処理はしない\n  - ブラウザのauto-fillを活用できる\n  - アプリもしくは別のWeb sitesと連携して支払いを行う仕組みを実現する\n- Usage\n  - `methods`: Payment Method Identifiers\n    - basic-cardを指定するとブラウザに登録されている生のクレカ情報を使う\n    - URL: Payment Apps (Android Pay, AliPay,, Samsung Pay)\n  - `details`: 商品詳細情報\n    - 配送方法なども指定できる\n  - `options`: オプション\n    - `requestShipping`: `boolean` 住所を指定できるオプション\n    - `requestpayermail`\n- 決済代行業者(PSP)との連携について\n  - 今後PSPごとにPayments APIを使う場合のガイドラインなどが発表されていくようになるかもだけどまだその段階にはない\n  - 2020年までにクレジットカードを悪用された犯罪を減らしていく国の取り組みがある\n  - 基本はクレジットカード情報を非保持\n  - もし、保持する場合はPCI DSSに準拠する必要がある\n    - 400弱のチェック項目がある\n    - これを各サービスが準拠するのは難しい\n    - そこでPSP\n  - PCIの種類\n    - PCI DSS\n    - PCI SAQ A-EP　チェック項目は約200。\n    - PCI SAQ A: 最っも簡単なモノ。チェック項目は約20。\n    - EC sitesがPCI DSSを準拠することは少ない\n  - PSPとMerchantとBrowserの連携\n    - 簡単な方法は,リンク型\n      - iframeやlinkを使ってPSPが提供しているフォームを組み込む\n      - こうすることによりカード情報が非保持になるのでPCI SAQ Aを満たせば良いだけになる→できそう！\n　　- トークン型\n      - PCI SAQ A or A-EP\n  - Tokenization\n    - カード番号の代わりにそのトランザクションでだけ使えるトークンと呼ばれる文字列を発行\n    - カード番号そのものをハッシュや暗号化したものではないので不可逆\n    - 他の目的で転用でいないため、漏洩しても被害は最小限。\n    - トークンを使うことでPCI DSS適用範囲を縮小できる(SAQ A)。\n    - これをどうやってWebに組み込めばよいか?\n  - ネイティブペイメントアプリ\n    - プラットフォーム依存の仕様\n    - bobpay.xyz\n      - Integration Guide\n    - Androiid Chrome 60~\n    - 登場予定\n      - Alipay\n      - Samsung Pay\n      - これらはMerchantが使うものを指定し、ユーザに選択してもらう\n  - Payment Request APIとペイメントアプリの連携\n    - Browserがネイティブペイメントアプリを経由してトークンを取得する\n    - トークン発行をrequestするとPayment request APIにtokenが戻ってくる\n    - これによりMerchantはクレカ情報に触れなくてよくなる\n  - Pay with Google\n  - ウェブペイメントアプリ\n    - native paymenet appはユーザがinstallしてないと使えない\n    - 標準化進行中: Payment Handler API\n    - Web Payment AppsはService Workerを使う\n    - Service Workerを使って、Web sitesを開いていなくても、token発行イベントを出してWev Payment Apps経由でtokenを取得できる\n  - Browser compatibility\n    - Chrome for Android\n    - Chrome for Desktop 61~\n    - Chrome for iOS\n    - Samsung Internet Browser\n    - Microsoft Edge\n    - Mozilla Firefox\n    - Facebook\n    - iOS SafariではApple PayのApple Pay JSで似たようなことを実現できる\n    - WebkitではPayment RequestはUnder consideration\n    - g.co/PaymentRequestAPI\n- Payment Request APIを使える条件は?\n  - https必須\n- 入力内容によって送料を変えるといったことはできるか?\n  - 入力内容が更新されるとイベントが飛ぶので、よしなにできる\n- どの粒度でsites間、端末間で情報を同期できる?\n  - 住所は同期される\n  - クレカ情報は基本同期されない\n    - ただしGoogle Paymentsに保存されているクレカ情報はChromeから読める\n- WebViewを使うと各アプリから触れますよね?\n  - 正確にはChrome custom tab。これをChromeが立ち上げる。\n\n# クレカの Tokenization by @keketa\n\n- PAY.JP\n- PAY ID\n- What's tokenization?\n  - カード番号を他のデータに置い変えて処理を行うこと\n- Why tokenization?\n  - カード情報事故増加によるセキュリティニーズの高まり\n  - 実行計画2017\n  - Android Pay/Apple PayなどセキュリティとUXを向上されるサービスの登場\n- PCI-DSS\n  - カードブランド各社によって設立された\n  - 訪問監査\n  - SAQ\n    - 訪問監査なし、自己問診んにより準拠を証明する方法\n- Tokenization分類\n  - 可逆・不可逆\n  - 1度きり・複数回利用可\n  - ブランド・アクワイアラ・イシュア・共同NW・PSPにそれぞれ役割がある\n- CDE(cardholder data environment)によるNW制御\n- FWアクセス制限、セキュアな通信、認証機構、強固な暗号化を用いたデータ暗号化鍵、鍵暗号化鍵による暗号化など\n- Android Pay/Apple Pay\n  - イシュアーを介したTokenization処理を活用したスマホ決済\n  - 端末にカード番号は残らず、トークン化された番号形式が保存される\n- Q&A\n  - tokenの失効させるフローは?\n    - API keyとセットでないと使えない前提\n    - Device Account Numberがもれるとどうなるか？token単体で利用できないのではという見解\n  - 実装によって脆弱なtokenができたりすると思うが、何か実装の基準みたいなものは?\n    - PCI-DSSに基づいた実装と、カード番号と発行させる実装を扱う環境を完全に分けている\n  - DANはDevice固有?\n    - Device固有だが、クレカのCVCのような認証機構によってダイナミックスが担保される?\n  - Identity界隈の人たちが言っているTokenizationとApple PayにおけるTokenizationは別???\n"
                                      }
                                    }
                                  ]
                                }
                              }
                            ]
                          }
                        },
                        {
                          "type": "tree",
                          "name": "09",
                          "object": {
                            "entries": [
                              {
                                "type": "tree",
                                "name": "08",
                                "object": {
                                  "entries": [
                                    {
                                      "type": "blob",
                                      "name": "graphql-schema-ts-type-definitions.md",
                                      "object": {
                                        "commitUrl": "https://github.com/rkaneko/weblog/commit/440b3534cb5aed7327cc701207c279232f0928fe",
                                        "text": "GraphQL schema.json TypeScript type definitions\n===\n\n### Prerequisistes\n\n```bash\n$ npm i @types/graphql\n```\n\n### GraphQL schema.json TypeScript type definitions\n\nNormal JSON file's `d.ts` [How to Import json into TypeScript](https://hackernoon.com/import-json-into-typescript-8d465beded79)\n\n`graphql-schema-json.d.ts`\n\n```ts\ndeclare module \"*/schema.json\" {\n    import { IntrospectionQuery } from \"graphql\";\n    interface SchemaJson {\n        data: IntrospectionQuery;\n    }\n    const value: SchemaJson;\n    export = value;\n}\n```\n\nWhen you put an above file and edit `tsconfig.json` appropriately, you can import `schema.json` in ts files like this.\n\n```ts\nimport * as schemaJson from \"path/2/schema.json\";\n\nconsole.dir(schemaJson.data);\n```\n"
                                      }
                                    }
                                  ]
                                }
                              }
                            ]
                          }
                        }
                      ]
                    }
                  }
                ]
              }
            }
          ]
        }
      }
    }
  }
}
